{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662f4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project = 'newfault'\n",
    "traindate = '2024-10-01'\n",
    "testdate = '2024-10-02'\n",
    "traindate_path = '/home/sdybing/gnss-picker/cnn_models_outputs/' + project + '_fq_train/models/traindate_' + traindate + '/'\n",
    "test_outputs_path = traindate_path + 'data/'\n",
    "figure_save_dir = traindate_path + 'figures/'\n",
    "realdata_dir = '/home/sdybing/gnss-picker/data/realdata/summer23/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96a1e4b-26ad-48bd-b77e-44aa049b48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtest_data = np.load(test_outputs_path + testdate + '_realtest_orig_data.npy')\n",
    "realtest_metadata = np.load(test_outputs_path + testdate + '_realtest_metadata.npy')\n",
    "realtest_target = np.load(test_outputs_path + testdate + '_realtest_target.npy')\n",
    "realtest_predictions = np.load(test_outputs_path + testdate + '_realtest_predictions.npy')\n",
    "rows_w_eqs = np.load(realdata_dir + 'real_metadata_rembad_rows_w_eqs.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c2363a-95db-4abd-ab69-f31a51005016",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_realtest = len(realtest_predictions)\n",
    "thresholds = np.arange(0, 1.005, 0.005)\n",
    "test_thresholds = [0, 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13b864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994155, 128, 3)\n",
      "(994155, 7)\n",
      "(994155, 128)\n",
      "(994155, 128)\n",
      "(2123,)\n"
     ]
    }
   ],
   "source": [
    "print(realtest_data.shape)\n",
    "print(realtest_metadata.shape)\n",
    "print(realtest_target.shape)\n",
    "print(realtest_predictions.shape)\n",
    "print(rows_w_eqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d31bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata columns:\n",
    "# 0: station name\n",
    "# 1: date sample came from\n",
    "# 2: sample start time\n",
    "# 3: sample end time\n",
    "# 4: random n counter\n",
    "# 5: sample P-wave arrives at (gauss pos)\n",
    "# 6: earthquake magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9fb9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### -------------------- CLASSIFICATION TESTS -------------------- #####\n",
    "\n",
    "# Decision threshold evaluation\n",
    "\n",
    "thresholds = np.arange(0, 1.005, 0.005)\n",
    "# thresholds = np.arange(0, 1.1, 0.1)\n",
    "# thresholds = np.arange(0, 1, 0.1)\n",
    "test_thresholds = [0.6]\n",
    "\n",
    "# Use np.where to see whether anywhere in test_predictions is > threshold\n",
    "# If there is a value that's >, the 'result' of the array is 1. If not 0\n",
    "# Then compare these 1s and 0s to the target array value for PAR\n",
    "\n",
    "accuracies = []\n",
    "accuracies_per = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "F1s = []\n",
    "\n",
    "TP_pert = []\n",
    "TN_pert = []\n",
    "FP_pert = []\n",
    "FN_pert = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Threshold: ' + str(threshold))\n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    # Convert the predictions arrays to single ones and zeroes\n",
    "    \n",
    "    pred_binary = np.zeros(len(realtest_predictions))\n",
    "    iterate = np.arange(0,len(realtest_predictions),1)\n",
    "    for k in iterate:\n",
    "        # print('Prediction: ' + str(realtest_predictions[k]))\n",
    "        i = np.where(realtest_predictions[k] >= threshold)[0]\n",
    "        # print(i)\n",
    "        if len(i) == 0:\n",
    "            pred_binary[k] = 0\n",
    "        elif len(i) > 0:\n",
    "            pred_binary[k] = 1\n",
    "\n",
    "#     print('Predictions: ')\n",
    "#     print(pred_binary)\n",
    "#     print(pred_binary.shape)\n",
    "    \n",
    "    # Convert the target arrays to single ones and zeroes\n",
    "    \n",
    "    targ_binary = np.zeros(len(realtest_target)) # Need to make this ones at indices in rows_w_eqs\n",
    "    for idx in range(len(targ_binary)):\n",
    "        if idx in rows_w_eqs:\n",
    "            targ_binary[idx] = 1\n",
    "    \n",
    "#     print('Targets: ')\n",
    "#     print(targ_binary)\n",
    "    \n",
    "    # Calculating the accuracy, precision, recall, and F1\n",
    "    \n",
    "    num_preds = len(realtest_predictions) # total number of predictions. Amanda did 50\n",
    "    correct_preds = []\n",
    "    wrong_preds = []\n",
    "    true_pos = []\n",
    "    true_neg = []\n",
    "    false_pos = []\n",
    "    false_neg = []\n",
    "    \n",
    "    for i in iterate:\n",
    "        pred = pred_binary[i]\n",
    "        targ = targ_binary[i]\n",
    "        if pred == targ: # add one to list of correct predictions if matching\n",
    "            correct_preds.append(1)\n",
    "            if pred == 1 and targ == 1:\n",
    "                true_pos.append(1)\n",
    "            elif pred == 0 and targ == 0:\n",
    "                true_neg.append(1)\n",
    "        elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "            wrong_preds.append(1)\n",
    "            if pred == 1 and targ == 0:\n",
    "                false_pos.append(1)\n",
    "            elif pred == 0 and targ == 1:\n",
    "                false_neg.append(1)\n",
    "    \n",
    "    num_correct_preds = len(correct_preds)\n",
    "    num_wrong_preds = len(wrong_preds)\n",
    "    num_true_pos = len(true_pos)\n",
    "    num_true_neg = len(true_neg)\n",
    "    num_false_pos = len(false_pos)\n",
    "    num_false_neg = len(false_neg)\n",
    "    \n",
    "    TP_pert.append(num_true_pos)\n",
    "    TN_pert.append(num_true_neg)\n",
    "    FP_pert.append(num_false_pos)\n",
    "    FN_pert.append(num_false_neg)\n",
    "    \n",
    "#     print('Threshold: ' + str(threshold))\n",
    "    print('Correct preds: ' + str(num_correct_preds))\n",
    "    print('Wrong preds: ' + str(num_wrong_preds))\n",
    "    print('True pos: ' + str(np.round(100*num_true_pos/num_preds,2)) + '%')\n",
    "    print('True neg: ' + str(np.round(100*num_true_neg/num_preds,2)) + '%')\n",
    "    print('False pos: ' + str(np.round(100*num_false_pos/num_preds,2)) + '%')\n",
    "    print('False neg: ' + str(np.round(100*num_false_neg/num_preds,2)) + '%')\n",
    "    \n",
    "    accuracy = num_correct_preds / num_preds\n",
    "    accuracy_per = (num_correct_preds / num_preds) * 100\n",
    "    print('Accuracy: ' + str(np.round(accuracy_per,1)) + '%')\n",
    "    \n",
    "    if num_true_pos == 0  and num_false_pos == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = num_true_pos / (num_true_pos + num_false_pos)\n",
    "    \n",
    "    if num_true_pos == 0 and num_false_neg == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = num_true_pos / (num_true_pos + num_false_neg)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2 * ((precision * recall) / (precision + recall))\n",
    "        \n",
    "    accuracies.append(accuracy)\n",
    "    accuracies_per.append(accuracy_per)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    F1s.append(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be50049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TP_pert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4831b-67fb-474f-a145-28e4cec4d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(test_outputs_path + 'real_testing/classification_stats/thresholds.npy', thresholds)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/accuracies.npy', accuracies)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/precisions.npy', precisions)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/recalls.npy', recalls)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/F1s.npy', F1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(test_outputs_path + 'real_testing/classification_stats/TNnum.npy', TN_pert)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/FPnum.npy', FP_pert)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/TPnum.npy', TP_pert)\n",
    "np.save(test_outputs_path + 'real_testing/classification_stats/FNnum.npy', FN_pert)\n",
    "\n",
    "# np.savetxt(test_outputs_path + 'realtestdata_rembad_accuracies_percentage_txt.txt', accuracies_per)\n",
    "# np.savetxt(test_outputs_path + 'realtestdata_rembad_thresholds_txt.txt', thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cd1e5a-2f08-4cbd-988d-75cf04eccbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.load(test_outputs_path + 'real_testing/classification_stats/thresholds.npy')\n",
    "accuracies = np.load(test_outputs_path + 'real_testing/classification_stats/accuracies.npy')\n",
    "precisions = np.load(test_outputs_path + 'real_testing/classification_stats/precisions.npy')\n",
    "recalls = np.load(test_outputs_path + 'real_testing/classification_stats/recalls.npy')\n",
    "F1s = np.load(test_outputs_path + 'real_testing/classification_stats/F1s.npy')\n",
    "\n",
    "TN_pert = np.load(test_outputs_path + 'real_testing/classification_stats/TNnum.npy')\n",
    "FP_pert = np.load(test_outputs_path + 'real_testing/classification_stats/FPnum.npy')\n",
    "TP_pert = np.load(test_outputs_path + 'real_testing/classification_stats/TPnum.npy')\n",
    "FN_pert = np.load(test_outputs_path + 'real_testing/classification_stats/FNnum.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe73d78-a959-4f61-a862-13e909f5b79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99786754]\n"
     ]
    }
   ],
   "source": [
    "r = np.where(thresholds == 0.905)[0]\n",
    "print(accuracies[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "900fa4a7-9196-4698-ac99-07a3983b5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[0.91  0.915 0.925]\n",
      "[0.99786754 0.99786754 0.99786552]\n"
     ]
    }
   ],
   "source": [
    "e = np.where(precisions == np.max(precisions))[0]\n",
    "print(np.max(precisions))\n",
    "print(thresholds[e])\n",
    "print(accuracies[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02419dd9-8216-4c8e-bc02-6864840ccf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[0.    0.005 0.01  0.015 0.02  0.025]\n",
      "[0.00213548 0.00213548 0.00213548 0.00213548 0.00213649 0.00214554]\n"
     ]
    }
   ],
   "source": [
    "e = np.where(recalls == np.max(recalls))[0]\n",
    "print(np.max(recalls))\n",
    "print(thresholds[e])\n",
    "print(accuracies[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40eb4161-5be2-49cd-b7c5-fb442eb1a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026223776223776224\n",
      "[0.735]\n",
      "[0.9977589]\n"
     ]
    }
   ],
   "source": [
    "e = np.where(F1s == np.max(F1s))[0]\n",
    "print(np.max(F1s))\n",
    "print(thresholds[e])\n",
    "print(accuracies[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca0c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905\n"
     ]
    }
   ],
   "source": [
    "# Find threshold with highest accuracy\n",
    "\n",
    "acc0 = 0\n",
    "\n",
    "for idx in range(len(accuracies)):\n",
    "    acc = accuracies[idx]\n",
    "    if acc > acc0:\n",
    "        acc0 = acc\n",
    "        best_thresh = thresholds[idx] # Only updates when it hits a higher accuracy\n",
    "        \n",
    "print(best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2b7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6.5), dpi = 400)\n",
    "ax1 = plt.subplot(111)\n",
    "\n",
    "line1, = ax1.plot(thresholds, TN_pert, label = 'True neg', color = 'blue', lw = 3)\n",
    "line2, = ax1.plot(thresholds, FP_pert, label = 'False pos', color = 'red', lw = 3)\n",
    "ax1.set_xlim(0,0.1)\n",
    "ax2 = ax1.twinx()\n",
    "line3, = ax2.plot(thresholds, TP_pert, label = 'True pos', color = 'green', lw = 3)\n",
    "line4, = ax2.plot(thresholds, FN_pert, label = 'False neg', color = 'orange', lw = 3)\n",
    "ax2.set_ylim(0,3000)\n",
    "ax1.set_ylim(0,1070000)\n",
    "ax1.set_xlabel('Threshold', fontsize = 14)\n",
    "ax1.set_ylabel('Number of cases - TN & FP', fontsize = 14)\n",
    "ax2.set_ylabel('Number of cases - TP & FN', fontsize = 14)\n",
    "ax1.tick_params(labelsize = 14)\n",
    "ax2.tick_params(labelsize = 14)\n",
    "# ax1.axvline(x = 0.025)\n",
    "# ax1.legend(fontsize = 14)\n",
    "# ax2.legend(fontsize = 14)\n",
    "# plt.title('Result Case Count', fontsize = 16)\n",
    "\n",
    "# lines = [line2, line3, line1, line4]\n",
    "# labels = [line.get_label() for line in lines]\n",
    "# ax1.legend(lines, labels, loc = (0.025, 0.025), ncol = 1, fontsize = 14)\n",
    "\n",
    "# plt.show();\n",
    "plt.savefig('/home/sdybing/gnss-picker/manuscript_figures/real_testdata_crossover_zoomed.jpg', format = 'JPG', facecolor = 'white')\n",
    "plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de28217-bd30-4e41-ba3a-6778d6514c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6.5), dpi = 400)\n",
    "ax1 = plt.subplot(111)\n",
    "\n",
    "line1, = ax1.plot(thresholds, TN_pert, label = 'True neg', color = 'blue', lw = 2)\n",
    "line2, = ax1.plot(thresholds, FP_pert, label = 'False pos', color = 'red', lw = 2)\n",
    "ax1.set_xlim(0,1)\n",
    "ax2 = ax1.twinx()\n",
    "line3, = ax2.plot(thresholds, TP_pert, label = 'True pos', color = 'green', lw = 2)\n",
    "line4, = ax2.plot(thresholds, FN_pert, label = 'False neg', color = 'orange', lw = 2)\n",
    "ax2.set_ylim(0,3000)\n",
    "ax1.set_ylim(0,1070000)\n",
    "ax1.set_xlabel('Threshold', fontsize = 14)\n",
    "ax1.set_ylabel('Number of cases - TN & FP', fontsize = 14)\n",
    "ax2.set_ylabel('Number of cases - TP & FN', fontsize = 14)\n",
    "ax1.tick_params(labelsize = 14)\n",
    "ax2.tick_params(labelsize = 14)\n",
    "# ax1.legend(fontsize = 14)\n",
    "# ax2.legend(fontsize = 14)\n",
    "# plt.title('Result Case Count', fontsize = 16)\n",
    "\n",
    "lines = [line2, line3, line1, line4]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc = (-0.09, -0.22), ncol = 4, fontsize = 14)\n",
    "\n",
    "plt.subplots_adjust(bottom = 0.2, right = 0.85)\n",
    "\n",
    "# plt.show();\n",
    "plt.savefig('/home/sdybing/gnss-picker/manuscript_figures/real_testdata_crossover.jpg', format = 'JPG', facecolor = 'white')\n",
    "plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f775c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "#plt.figure(figsize = (8,5))\n",
    "# plt.scatter(thresholds,accuracies)\n",
    "plt.plot(thresholds, accuracies_per, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('Accuracy (%)', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,100)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Accuracy Percentage', fontsize = 18)\n",
    "plt.show();\n",
    "# plt.savefig(figure_save_dir + '18a_realtestdata_accuracies_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "# plt.close();\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "#plt.figure(figsize = (8,5))\n",
    "plt.plot(thresholds, precisions, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('Precision', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Precision', fontsize = 18)\n",
    "plt.show();\n",
    "# plt.savefig(figure_save_dir + '18b_realtestdata_precisions_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "# plt.close();\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "#plt.figure(figsize = (8,5))\n",
    "plt.plot(thresholds, recalls, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('Recall', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Recall', fontsize = 18)\n",
    "plt.show();\n",
    "# plt.savefig(figure_save_dir + '18c_realtestdata_recalls_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "# plt.close()\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "#plt.figure(figsize = (8,5))\n",
    "plt.plot(thresholds, F1s, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('F1', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('F1', fontsize = 18)\n",
    "plt.show();\n",
    "# plt.savefig(figure_save_dir + '18d_realtestdata_F1s_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "# plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3f03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5), dpi = 400, facecolor = 'white')\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.text(x = -0.3, y = 0.9, s = '(a)', fontsize = 22)\n",
    "plt.grid(lw = 0.5, zorder = 0)\n",
    "plt.plot(thresholds, accuracies, linewidth = 2)\n",
    "plt.ylabel('Accuracy', fontsize = 12)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.axvline(best_thresh, color = 'red', linestyle = '--', alpha = 0.6, label = 'Max accuracy at threshold of ' + str(best_thresh))\n",
    "plt.tick_params(axis = 'both', bottom = False, labelbottom = False)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.legend(loc = (0.55,-1.45))\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.text(x = -0.3, y = 0.9, s = '(b)', fontsize = 22)\n",
    "plt.grid(lw = 0.5, zorder = 0)\n",
    "plt.plot(thresholds, precisions, linewidth = 2)\n",
    "plt.ylabel('Precision', fontsize = 12)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.tick_params(axis = 'both', bottom = False, labelbottom = False)\n",
    "plt.axvline(best_thresh, color = 'red', linestyle = '--', alpha = 0.6)\n",
    "plt.yticks(fontsize = 11)\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.text(x = -0.3, y = 0.9, s = '(c)', fontsize = 22)\n",
    "plt.grid(lw = 0.5, zorder = 0)\n",
    "plt.plot(thresholds, recalls, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 12)\n",
    "plt.ylabel('Recall', fontsize = 12)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8], fontsize = 11)\n",
    "plt.axvline(best_thresh, color = 'red', linestyle = '--', alpha = 0.6)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.text(x = -0.3, y = 0.9, s = '(d)', fontsize = 22)\n",
    "plt.grid(lw = 0.5, zorder = 0)\n",
    "plt.plot(thresholds, F1s, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 12)\n",
    "plt.ylabel('F1', fontsize = 12)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8], fontsize = 11)\n",
    "plt.axvline(best_thresh, color = 'red', linestyle = '--', alpha = 0.6)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0, wspace = 0.32, bottom = 0.2)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.savefig('/home/sdybing/gnss-picker/manuscript_figures/real_testdata_classification.jpg', format = 'JPG')\n",
    "plt.savefig('/home/sdybing/gnss-picker/manuscript_figures/Figure_8.png', format = 'PNG')\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890d345-76db-4ef0-9dcf-0b569a4f752a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
