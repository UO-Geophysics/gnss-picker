{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18ca495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# project = 'newfault'\n",
    "# traindate = '2024-10-01'\n",
    "# testdate = '2024-10-02'\n",
    "# traindate_path = '/home/sdybing/gnss-picker/cnn_models_outputs/' + project + '_fq_train/models/traindate_' + traindate + '/'\n",
    "# test_outputs_path = traindate_path + 'data/'\n",
    "# figure_save_dir = traindate_path + 'figures/'\n",
    "\n",
    "# fqtest_data = np.load(test_outputs_path + testdate + '_fqtest_orig_data.npy')\n",
    "# fqtest_metadata = np.load(test_outputs_path + testdate + '_fqtest_metadata.npy')\n",
    "# fqtest_target = np.load(test_outputs_path + testdate + '_fqtest_target.npy')\n",
    "# fqtest_predictions = np.load(test_outputs_path + testdate + '_fqtest_predictions.npy')\n",
    "\n",
    "# results = np.load(test_outputs_path + 'fakequakes_testing/fqtest_metadata_withresults.npy')\n",
    "\n",
    "# num_fqtest = len(fqtest_predictions)\n",
    "# best_thresh = 0.135 # From code 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eea4210c-659f-4bf2-a6c7-0ee25874fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from obspy.geodetics import gps2dist_azimuth\n",
    "\n",
    "path = '/home/sdybing/gnss-picker/cnn_models_outputs/newfault_fq_train/models/traindate_2024-10-01/data/'\n",
    "\n",
    "data = np.load(path + '2024-10-16_gfast_bigger_fqtest_orig_data.npy')\n",
    "metadata = np.load(path + 'bigger_pgd_test_metadata_with_hypdists.npy')\n",
    "targets = np.load(path + '2024-10-16_gfast_bigger_fqtest_target.npy')\n",
    "predictions = np.load(path + '2024-10-16_gfast_bigger_fqtest_predictions.npy')\n",
    "\n",
    "# data = np.load(path + '2024-10-16_gfast_bigger_fqtest_orig_data.npy')\n",
    "# metadata = np.load(path + 'bigger_pgd_test_metadata_with_hypdists.npy')\n",
    "\n",
    "# Columns:\n",
    "\n",
    "# 0: FQ rupture name\n",
    "# 1: station name\n",
    "# 2: magnitude\n",
    "# 3: P-arrival index\n",
    "# 4: FQ rupture hypocenter lat\n",
    "# 5: FQ rupture hypocenter lon\n",
    "# 6: FQ rupture hypocenter depth (km)\n",
    "# 7: station lat\n",
    "# 8: station lon\n",
    "# 9: hypocentral distance (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17f1e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newfault.001022' 'IMPS' '6.2624' '41' '35.692601' '-117.545871' '0.8'\n",
      " '34.157563' '-115.1451' '277688.3029341335']\n",
      "(20683, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(metadata[0])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5e7a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20683,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating PGDs from the waveforms - HAVE TO USE ORIGINAL, NOT NORM, DATA\n",
    "\n",
    "pgds = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    n_data = data[i,:,0]\n",
    "    e_data = data[i,:,1]\n",
    "    z_data = data[i,:,2]\n",
    "    pgd = np.max(np.sqrt((n_data)**2+(e_data)**2+(z_data)**2))\n",
    "    pgds.append(pgd)\n",
    "\n",
    "print(np.array(pgds).shape)\n",
    "np.save(path + 'gfast_bigger_test_metadata_pgds.npy', np.array(pgds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b8f42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20683\n",
      "20683\n",
      "20683\n",
      "20683\n"
     ]
    }
   ],
   "source": [
    "# Calculating SNRs from the waveforms using the target Gaussian peak as the arrival time\n",
    "\n",
    "targets_count = []\n",
    "\n",
    "SNRs_N = []\n",
    "SNRs_E = []\n",
    "SNRs_Z = []\n",
    "mean_SNRs = []\n",
    "\n",
    "for idx in range(len(targets)):\n",
    "    \n",
    "    target_min = min(targets[idx,:])\n",
    "    target_max = max(targets[idx,:])\n",
    "    target_range = target_max - target_min\n",
    "    \n",
    "    if target_range != 0:\n",
    "        target_max_idx = np.argmax(targets[idx,:])\n",
    "        targets_count.append(target_max_idx)\n",
    "    \n",
    "        p_arrival_index = int(target_max_idx) # The index in the sample that the P-wave arrives at\n",
    "        if p_arrival_index != float(metadata[idx,3]):\n",
    "            print(idx)\n",
    "        \n",
    "        '''\n",
    "        In this section, I calculate the signal-to-noise ratio of the data. I \n",
    "        aim to use a window of 20 seconds before the P-wave arrival time as the \n",
    "        noise, and a window of 20 seconds after the P-wave arrival time as the \n",
    "        signal. I take the standard deviation of these segments and divide \n",
    "        signal/noise (or after/before) to get the SNR.\n",
    "        \n",
    "        Sometimes the P-wave arrival time is too close to the start or end of the\n",
    "        sample, and this causes issues. I've added conditions for these cases.\n",
    "        '''\n",
    "        \n",
    "        preeq_std_end = p_arrival_index # The end of the 20 second 'noise' section before the earthquake is the P-wave arrival index\n",
    "        \n",
    "        if preeq_std_end <= 10: # Ask Diego if this is reasonable # Try 10\n",
    "        \n",
    "            # If P-wave pick is at zero - can't calculate a pre-eq standard deviation. \n",
    "            # OR the P-wave pick is too close to zero, it throws off the SNR values by a LOT.\n",
    "            \n",
    "            SNR_N = 'nan' # Just skip it (at least 10 cases for Z component with weird SNRs - one over 10,000!)\n",
    "            SNR_E = 'nan'\n",
    "            SNR_Z = 'nan'\n",
    "        \n",
    "        elif preeq_std_end > 10 and preeq_std_end <= 20: # If the pre-earthquake noise window is smaller than 20 seconds...\n",
    "            \n",
    "            preeq_std_start = 0\n",
    "            \n",
    "            posteq_std_start = p_arrival_index # Start the section for the \"signal\" at the P-wave arrival index\n",
    "            posteq_std_end = posteq_std_start + 20\n",
    "            # posteq_std_end = posteq_std_start + p_arrival_index # If the window before is less than 20 because the arrival time is less than 20, this makes the window after that same length\n",
    "            \n",
    "            std_before_N = np.std(data[idx,preeq_std_start:preeq_std_end,0]) # Take the standard deviation of the sections for each component\n",
    "            std_after_N = np.std(data[idx,posteq_std_start:posteq_std_end,0])\n",
    "            std_before_E = np.std(data[idx,preeq_std_start:preeq_std_end,1])\n",
    "            std_after_E = np.std(data[idx,posteq_std_start:posteq_std_end,1])\n",
    "            std_before_Z = np.std(data[idx,preeq_std_start:preeq_std_end,2])\n",
    "            std_after_Z = np.std(data[idx,posteq_std_start:posteq_std_end,2])\n",
    "            \n",
    "            if std_before_N == 0 or std_before_E == 0 or std_before_Z == 0: # If any of the denominators are zeros, we get 'inf' in the results\n",
    "                \n",
    "                SNR_N = 'nan' # Skip 'em\n",
    "                SNR_E = 'nan'\n",
    "                SNR_Z = 'nan'\n",
    "                \n",
    "            else: # Calculate the SNR\n",
    "                \n",
    "                SNR_N = std_after_N / std_before_N\n",
    "                SNR_E = std_after_E / std_before_E\n",
    "                SNR_Z = std_after_Z / std_before_Z\n",
    "        \n",
    "        elif preeq_std_end > 20 and preeq_std_end <= 108: # Standard case where the P-wave arrival is nicely in the middle somewhere\n",
    "            \n",
    "            preeq_std_start = preeq_std_end - 20\n",
    "            \n",
    "            posteq_std_start = p_arrival_index\n",
    "            posteq_std_end = posteq_std_start + 20\n",
    "        \n",
    "            std_before_N = np.std(data[idx,preeq_std_start:preeq_std_end,0]) # Take the standard deviation of the sections for each component\n",
    "            std_after_N = np.std(data[idx,posteq_std_start:posteq_std_end,0])\n",
    "            std_before_E = np.std(data[idx,preeq_std_start:preeq_std_end,1])\n",
    "            std_after_E = np.std(data[idx,posteq_std_start:posteq_std_end,1])\n",
    "            std_before_Z = np.std(data[idx,preeq_std_start:preeq_std_end,2])\n",
    "            std_after_Z = np.std(data[idx,posteq_std_start:posteq_std_end,2])\n",
    "            \n",
    "            if std_before_N == 0 or std_before_E == 0 or std_before_Z == 0:\n",
    "                \n",
    "                SNR_N = 'nan'\n",
    "                SNR_E = 'nan'\n",
    "                SNR_Z = 'nan'\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                SNR_N = std_after_N / std_before_N\n",
    "                SNR_E = std_after_E / std_before_E\n",
    "                SNR_Z = std_after_Z / std_before_Z\n",
    "            \n",
    "        elif preeq_std_end > 108 and preeq_std_end < 128: # End edge case - the \"signal\" period is less than 20 seconds long\n",
    "            \n",
    "            preeq_std_start = preeq_std_end - 20\n",
    "            \n",
    "            posteq_std_start = p_arrival_index # Should the below be 127 instead??\n",
    "            posteq_std_end = posteq_std_start + (128 - p_arrival_index) # Make the signal period end at the end of the sample at 128 to avoid errors\n",
    "        \n",
    "            std_before_N = np.std(data[idx,preeq_std_start:preeq_std_end,0]) # Take the standard deviation of the sections for each component\n",
    "            std_after_N = np.std(data[idx,posteq_std_start:posteq_std_end,0])\n",
    "            std_before_E = np.std(data[idx,preeq_std_start:preeq_std_end,1])\n",
    "            std_after_E = np.std(data[idx,posteq_std_start:posteq_std_end,1])\n",
    "            std_before_Z = np.std(data[idx,preeq_std_start:preeq_std_end,2])\n",
    "            std_after_Z = np.std(data[idx,posteq_std_start:posteq_std_end,2])\n",
    "            \n",
    "            if std_before_N == 0 or std_before_E == 0 or std_before_Z == 0:\n",
    "                \n",
    "                SNR_N = 'nan'\n",
    "                SNR_E = 'nan'\n",
    "                SNR_Z = 'nan'\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                SNR_N = std_after_N / std_before_N\n",
    "                SNR_E = std_after_E / std_before_E\n",
    "                SNR_Z = std_after_Z / std_before_Z\n",
    "            \n",
    "        else: # Covers if the pick is exactly at 128, the end of the sample.\n",
    "            \n",
    "            # Can't get a post-eq std because the earthquake arrives at the end of the sample\n",
    "            \n",
    "            SNR_N = 'nan' # Skip 'em (5 cases)\n",
    "            SNR_E = 'nan'\n",
    "            SNR_Z = 'nan'\n",
    "            \n",
    "        '''\n",
    "        Add the calculated SNRs (or 'nan's for issues) to the lists.\n",
    "        '''\n",
    "        \n",
    "        # if SNR_N == 0:\n",
    "            \n",
    "        #     print(idx)\n",
    "            \n",
    "        SNRs_N.append(SNR_N)\n",
    "        SNRs_E.append(SNR_E)\n",
    "        SNRs_Z.append(SNR_Z)\n",
    "        \n",
    "    elif target_range == 0:\n",
    "        \n",
    "        SNRs_N.append('nan')\n",
    "        SNRs_E.append('nan')\n",
    "        SNRs_Z.append('nan')\n",
    "        \n",
    "    try:\n",
    "        mean_SNR = (SNR_N + SNR_E + SNR_Z) / 3\n",
    "    except:\n",
    "        mean_SNR = 'nan'\n",
    "        \n",
    "    mean_SNRs.append(mean_SNR)\n",
    "\n",
    "print(len(SNRs_N))\n",
    "print(len(SNRs_E))\n",
    "print(len(SNRs_Z))\n",
    "print(len(mean_SNRs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f048ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path + 'gfast_bigger_test_SNRs_N.npy', np.array(SNRs_N))\n",
    "np.save(path + 'gfast_bigger_test_SNRs_E.npy', np.array(SNRs_E))\n",
    "np.save(path + 'gfast_bigger_test_SNRs_Z.npy', np.array(SNRs_Z))\n",
    "np.save(path + 'gfast_bigger_test_mean_SNRs.npy', np.array(mean_SNRs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db2fd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN mags removed: 0\n",
      "Number of good mags left: 20683\n",
      "Number of NaN PGDs removed: 0\n",
      "Number of good PGDs left: 20683\n",
      "Number of NaN SNRs removed: 738\n",
      "Number of good SNRs left: 19945\n"
     ]
    }
   ],
   "source": [
    "# Plots to check and make sure stuff looks right\n",
    "\n",
    "testmags = []\n",
    "for ii in range(len(metadata)):\n",
    "    mag = metadata[ii,2]\n",
    "    if mag == 'nan':\n",
    "        testmags.append(np.nan)\n",
    "    else:\n",
    "        testmags.append(float(mag))\n",
    "        \n",
    "testsnrs = []\n",
    "for iii in range(len(SNRs_N)):\n",
    "    snr = SNRs_N[iii]\n",
    "    if snr == 'nan':\n",
    "        testsnrs.append(np.nan)\n",
    "    else:\n",
    "        testsnrs.append(float(snr))\n",
    "        \n",
    "testpgds = []\n",
    "for iiii in range(len(pgds)):\n",
    "    pgd = pgds[iiii]\n",
    "    if pgd == 'nan':\n",
    "        testpgds.append(np.nan)\n",
    "    else:\n",
    "        testpgds.append(float(pgd))\n",
    "\n",
    "logsnrs = np.log10(testsnrs)\n",
    "logpgds = np.log10(testpgds)\n",
    "\n",
    "# Remove NaNs to be able to make this quick plot\n",
    "fixmags = []\n",
    "numremovedmags = []\n",
    "for iv in range(len(testmags)):\n",
    "    mag = testmags[iv]\n",
    "    if np.isnan(mag):\n",
    "        numremovedmags.append(1)\n",
    "    else:\n",
    "        fixmags.append(mag)    \n",
    "print('Number of NaN mags removed: ' + str(len(numremovedmags)))\n",
    "print('Number of good mags left: ' + str(len(fixmags)))\n",
    "\n",
    "fixpgds = []\n",
    "numremovedpgds = []\n",
    "for iv in range(len(logpgds)):\n",
    "    pgd = logpgds[iv]\n",
    "    if np.isnan(pgd):\n",
    "        numremovedpgds.append(1)\n",
    "    else:\n",
    "        fixpgds.append(pgd)    \n",
    "print('Number of NaN PGDs removed: ' + str(len(numremovedpgds)))\n",
    "print('Number of good PGDs left: ' + str(len(fixpgds)))\n",
    "\n",
    "fixsnrs = []\n",
    "numremovedsnrs = []\n",
    "for iv in range(len(logsnrs)):\n",
    "    snr = logsnrs[iv]\n",
    "    if np.isnan(snr):\n",
    "        numremovedsnrs.append(1)\n",
    "    elif np.isinf(snr):\n",
    "        numremovedsnrs.append(1)\n",
    "    else:\n",
    "        fixsnrs.append(snr) \n",
    "print('Number of NaN SNRs removed: ' + str(len(numremovedsnrs)))\n",
    "print('Number of good SNRs left: ' + str(len(fixsnrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74627fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a list of indices for rows in the testing dataset that actually have earthquakes\n",
    "# # and for rows that the CNN correctly found those earthquakes\n",
    "\n",
    "# rows_w_eqs = []\n",
    "# correct_eq_inds = []\n",
    "\n",
    "# for idx in range(len(results)):\n",
    "    \n",
    "#     if results[idx,0] == 'nan':\n",
    "#         pass\n",
    "#     else:\n",
    "#         rows_w_eqs.append(idx)\n",
    "    \n",
    "#     if results[idx,3] == 'true pos':\n",
    "#         correct_eq_inds.append(idx)\n",
    "\n",
    "# print(len(rows_w_eqs))\n",
    "# print(len(correct_eq_inds))\n",
    "\n",
    "# np.save(test_outputs_path + 'fakequakes_testing/fqtest_data_rows_w_eqs.npy', np.array(rows_w_eqs))\n",
    "# np.save(test_outputs_path + 'fakequakes_testing/fqtest_data_rows_w_truepos_result.npy', np.array(correct_eq_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68ced28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20683, 15)\n"
     ]
    }
   ],
   "source": [
    "# Add the PGDs and SNRs to the results metadata array\n",
    "\n",
    "pgds_vector = np.array(pgds).reshape(len(pgds),1) \n",
    "a = np.append(metadata, pgds_vector, axis = 1) \n",
    "# Now contains rupture name, station name, magnitude, result, and PGD\n",
    "\n",
    "SNR_N_vector = np.array(SNRs_N).reshape(len(SNRs_N),1)\n",
    "SNR_E_vector = np.array(SNRs_E).reshape(len(SNRs_E),1)\n",
    "SNR_Z_vector = np.array(SNRs_Z).reshape(len(SNRs_Z),1)\n",
    "mean_SNR_vector = np.array(mean_SNRs).reshape(len(mean_SNRs),1)\n",
    "\n",
    "b = np.append(a, SNR_N_vector, axis = 1)\n",
    "c = np.append(b, SNR_E_vector, axis = 1)\n",
    "d = np.append(c, SNR_Z_vector, axis = 1)\n",
    "new_meta_array = np.append(d, mean_SNR_vector, axis = 1)\n",
    "\n",
    "print(new_meta_array.shape)\n",
    "\n",
    "# Columns:\n",
    "\n",
    "# 0: FQ rupture name\n",
    "# 1: station name\n",
    "# 2: magnitude\n",
    "# 3: P-arrival index\n",
    "# 4: FQ rupture hypocenter lat\n",
    "# 5: FQ rupture hypocenter lon\n",
    "# 6: FQ rupture hypocenter depth (km)\n",
    "# 7: station lat\n",
    "# 8: station lon\n",
    "# 9: hypocentral distance (m)\n",
    "# 10. PGD (m)\n",
    "# 11. SNRN\n",
    "# 12. SNRE\n",
    "# 13. SNRZ\n",
    "# 14. mean SNR\n",
    "\n",
    "np.save(path + 'pgd_bigger_test_metadata_w_hypdist_pgd_snrs.npy', new_meta_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "244ee8c3-5771-448c-9bc2-0c65f1686747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20683, 15)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_meta_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ba91-5ae4-42f1-8dec-4cf13682d2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a813d9-b58c-416e-bae3-2f1dde154363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d144e-f249-41b2-b9f1-799996ce993b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b55d62-c25f-4d0e-a71b-b9bed526dbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d59c42-7036-4f6e-86b4-0d990a7e6464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd679fc4-0046-4d4c-bc6e-36f476ca38b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e315f-8c62-42b8-ad9c-9e826e986d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d772b09-dbb2-4156-a8cd-d68380566e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7b02c-992e-4180-93d1-d4a9b3feeed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b43b4-24a9-4c6c-998e-4632a642c8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d523e-d232-45e7-a4b1-733f7d7fa9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34c7441f-2570-4074-b6cd-e3f3a810fbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newfault.001132' 'NHRG' '6.3333']\n",
      "['newfault.001132' 'NHRG' '6.3333' 'false neg']\n",
      "['newfault.001132' 'NHRG' '6.3333' 'false neg' '0.021478579700444114'\n",
      " '0.9797122685583282' '0.9337101853976125' '1.4218615090358038']\n"
     ]
    }
   ],
   "source": [
    "print(fqtest_metadata[0])\n",
    "print(results[0])\n",
    "print(new_meta_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4860107c-4308-4bda-885c-0cd2733410a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03591847188120218"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(new_meta_array[rows_w_eqs,4].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f2c3842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average magnitude of all FakeQuakes: 5.83\n",
      "Average magnitude of FakeQuakes the CNN correctly found: 6.89\n"
     ]
    }
   ],
   "source": [
    "all_eq_mags = np.asfarray(new_meta_array[rows_w_eqs, 2]) # The magnitudes of all of the earthquakes\n",
    "correct_eq_mags = np.asfarray(new_meta_array[correct_eq_inds, 2]) # The magnitudes of all the earthquakes the CNN found\n",
    "\n",
    "all_eq_avg_mag = np.mean(all_eq_mags)\n",
    "correct_eq_avg_mag = np.mean(correct_eq_mags)\n",
    "\n",
    "print('Average magnitude of all FakeQuakes: ' + str(round((all_eq_avg_mag),2)))\n",
    "print('Average magnitude of FakeQuakes the CNN correctly found: ' + str(round((correct_eq_avg_mag),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bde485a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PGD of all FakeQuakes: 3.59 cm\n",
      "Average PGD of FakeQuakes the CNN correctly found: 5.69 cm\n"
     ]
    }
   ],
   "source": [
    "all_eq_pgds = np.asfarray(new_meta_array[rows_w_eqs, 4]) # The PGDs of all of the earthquakes\n",
    "correct_eq_pgds = np.asfarray(new_meta_array[correct_eq_inds, 4]) # The PGDs of all the earthquakes the CNN found\n",
    "\n",
    "all_eq_avg_PGD = np.mean(all_eq_pgds)\n",
    "correct_eq_avg_PGD = np.mean(correct_eq_pgds)\n",
    "\n",
    "print('Average PGD of all FakeQuakes: ' + str(round((all_eq_avg_PGD * 100),2)) + ' cm')\n",
    "print('Average PGD of FakeQuakes the CNN correctly found: ' + str(round((correct_eq_avg_PGD * 100),2)) + ' cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1137a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the edge cases in the loop above, there are some nans in our SNR vectors.\n",
    "# We can't calculate averages with nans, so we need to find the rows with nans and\n",
    "# just remove them for the sake of this calculation.\n",
    "\n",
    "h = np.where(new_meta_array[rows_w_eqs,5] == 'nan') # Finds nans in SNRN column for all earthquakes\n",
    "non_nan_rows_w_eqs = np.delete(rows_w_eqs, h) # Removes those rows\n",
    "\n",
    "j = np.where(new_meta_array[correct_eq_inds,5] == 'nan') # Finds nans in SNRN for the earthquakes the CNN found\n",
    "non_nan_correct_eq_inds = np.delete(correct_eq_inds, j) # Removes those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3fcf4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average N-S component SNR of all FakeQuakes: 1.38\n",
      "Average N-S component SNR of FakeQuakes the CNN correctly found: 2.57\n",
      "-------------------------------------------------------------------\n",
      "Average E-W component SNR of all FakeQuakes: 1.51\n",
      "Average E-W component SNR of FakeQuakes the CNN correctly found: 3.12\n",
      "-------------------------------------------------------------------\n",
      "Average Z component SNR of all earthquakes: 1.07\n",
      "Average Z component SNR of earthquakes the CNN correctly found: 1.2\n"
     ]
    }
   ],
   "source": [
    "# Now I just grab the good SNRs out of the new metadata array and calculate the averages.\n",
    "\n",
    "all_eq_SNR_N = np.asfarray(new_meta_array[non_nan_rows_w_eqs, 5])\n",
    "all_eq_SNR_E = np.asfarray(new_meta_array[non_nan_rows_w_eqs, 6])\n",
    "all_eq_SNR_Z = np.asfarray(new_meta_array[non_nan_rows_w_eqs, 7])\n",
    "\n",
    "correct_eq_SNR_N = np.asfarray(new_meta_array[non_nan_correct_eq_inds, 5])\n",
    "correct_eq_SNR_E = np.asfarray(new_meta_array[non_nan_correct_eq_inds, 6])\n",
    "correct_eq_SNR_Z = np.asfarray(new_meta_array[non_nan_correct_eq_inds, 7])\n",
    "\n",
    "all_eq_SNR_N_avg = np.mean(all_eq_SNR_N)\n",
    "all_eq_SNR_E_avg = np.mean(all_eq_SNR_E)\n",
    "all_eq_SNR_Z_avg = np.mean(all_eq_SNR_Z)\n",
    "\n",
    "correct_eq_SNR_N_avg = np.mean(correct_eq_SNR_N)\n",
    "correct_eq_SNR_E_avg = np.mean(correct_eq_SNR_E)\n",
    "correct_eq_SNR_Z_avg = np.mean(correct_eq_SNR_Z)\n",
    "\n",
    "# print(len(all_eq_SNR_N))\n",
    "# print(len(correct_eq_SNR_N))\n",
    "\n",
    "print('Average N-S component SNR of all FakeQuakes: ' + str(round(all_eq_SNR_N_avg,2)))\n",
    "print('Average N-S component SNR of FakeQuakes the CNN correctly found: ' + str(round(correct_eq_SNR_N_avg,2)))\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "print('Average E-W component SNR of all FakeQuakes: ' + str(round(all_eq_SNR_E_avg,2)))\n",
    "print('Average E-W component SNR of FakeQuakes the CNN correctly found: ' + str(round(correct_eq_SNR_E_avg,2)))\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "print('Average Z component SNR of all earthquakes: ' + str(round(all_eq_SNR_Z_avg,2)))\n",
    "print('Average Z component SNR of earthquakes the CNN correctly found: ' + str(round(correct_eq_SNR_Z_avg,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92103e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
