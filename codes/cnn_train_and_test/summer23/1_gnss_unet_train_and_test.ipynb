{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47688b33",
   "metadata": {},
   "source": [
    "## Imports and Path/Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "153a368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from scipy import signal\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import os\n",
    "import X_gnss_unet_datagen_fn22 # Module with CNN/data generator code\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "project_name = 'july6' # Based on the name of the FakeQuakes project\n",
    "fq_dir = '/hdd/rc_fq/summer23/' # Where are the FakeQuakes stored? (The final .hdf5 file)\n",
    "noise_dir = '/home/sdybing/gnss-picker/data/noisedata/' # Where is the noise data stored?\n",
    "realdata_dir = '/home/sdybing/gnss-picker/data/realdata/summer23/' # Where is the real data stored?\n",
    "\n",
    "cnn_save_dir = '/home/sdybing/gnss-picker/cnn_models_outputs/' # Where do you want to save this code's outputs?\n",
    "project_save_dir = cnn_save_dir + project_name + '_fq_train/'\n",
    "base_figure_save_dir = project_save_dir + 'base_data_figures/' # Where to save the figures of just the data/generator tests\n",
    "models_path = project_save_dir + 'models/'\n",
    "if os.path.isdir(project_save_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(project_save_dir)\n",
    "    os.makedirs(base_figure_save_dir)\n",
    "    os.makedirs(models_path)\n",
    "    \n",
    "train = False # Do you want to train?\n",
    "drop = 1 # Drop?\n",
    "resume = 0 # Resume training\n",
    "large = 0.5 # Large unet\n",
    "fac = large\n",
    "epochs = 100 # How many epochs?\n",
    "std = 3 # How long do you want the Gaussian STD to be?\n",
    "sr = 1 # Sample rate (Hz)\n",
    "epsilon = 1e-6\n",
    "batch_size = 32\n",
    "load = True # Loading an old trained model?\n",
    "small_train = False # Train with a smaller amount of data to make sure code works?\n",
    "small_test = False # Test with a smaller amount of data to make sure code works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab514d",
   "metadata": {},
   "source": [
    "## Data Loading and Formatting\n",
    "\n",
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcd4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FakeQuakes...\n",
      "Loading FakeQuakes metadata...\n",
      "Loading noise...\n",
      "Loading real data...\n",
      "Loading real metadata...\n",
      "FakeQuakes shape: (916010, 768)\n",
      "Noise data shape: (916010, 768)\n",
      "Real data shape: (994155, 384)\n"
     ]
    }
   ],
   "source": [
    "# FakeQuakes waveform data\n",
    "print('Loading FakeQuakes...')\n",
    "fq_data = h5py.File(fq_dir + 'july6_128samps_fq_wvfm_data_formatted_nobadrupts.hdf5', 'r')\n",
    "fq_data = fq_data['data'][:,:]\n",
    "# old_fq_data = h5py.File(fq_dir + 'july6_128samps_fq_wvfm_data_formatted.hdf5', 'r')\n",
    "# old_fq_data = old_fq_data['data'][:,:]\n",
    "\n",
    "# FakeQuakes metadata\n",
    "print('Loading FakeQuakes metadata...')\n",
    "fq_metadata = np.load(fq_dir + 'july6_128samps_fq_wvfm_info_nobadrupts.npy')\n",
    "\n",
    "# Noise data\n",
    "print('Loading noise...')\n",
    "all_noise_data = h5py.File(noise_dir + 'summer23_128samps_all_noise_samples.hdf5', 'r')\n",
    "all_noise_data = all_noise_data['all_noise_samples'][:,:]\n",
    "\n",
    "# Normalized real waveform data\n",
    "print('Loading real data...')\n",
    "real_data = h5py.File(realdata_dir + 'demean_realdata_rembad.hdf5', 'r')\n",
    "real_data = real_data['demean_realdata_rembad'][:,:]\n",
    "\n",
    "# Real metadata\n",
    "print('Loading real metadata...')\n",
    "real_metadata = np.load(realdata_dir + 'real_metadata_rembad_w_gauss_pos_mag.npy')\n",
    "\n",
    "# Trim noise data to match length of FakeQuakes data\n",
    "noise_data = all_noise_data[:len(fq_data)]\n",
    "\n",
    "# Array of NaNs to use to match added noise in concatenation later\n",
    "nan_array = np.empty((len(fq_data), 3))\n",
    "nan_array[:] = np.NaN\n",
    "\n",
    "# Real data\n",
    "\n",
    "# Check shapes\n",
    "print('FakeQuakes shape: ' + str(fq_data.shape))\n",
    "print('Noise data shape: ' + str(noise_data.shape))\n",
    "print('Real data shape: ' + str(real_data.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725b7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding bad ruptures (data all zeros) - uses normalized fq_data file\n",
    "\n",
    "# inf_idxs = []\n",
    "\n",
    "# for idx in range(len(fq_data)):\n",
    "#     row = fq_data[idx]\n",
    "#     check_nan = np.isnan(row)\n",
    "#     check_inf = np.isinf(row)\n",
    "#     l = np.where(check_nan == True)[0]\n",
    "#     m = np.where(check_inf == True)[0]\n",
    "    \n",
    "#     if len(l) > 0 or len(m) > 0:\n",
    "#         inf_idxs.append(idx)\n",
    "\n",
    "# bad_rupts = []\n",
    "\n",
    "# for i in inf_idxs:\n",
    "# #     plt.plot(old_fq_data[i]) # Need to go back to 3a code and remove things that are all zero\n",
    "# #     print(fq_metadata[i][0])\n",
    "#     if fq_metadata[i][0] in bad_rupts:\n",
    "#         pass\n",
    "#     else:\n",
    "#         bad_rupts.append(fq_metadata[i][0])\n",
    "\n",
    "# np.save('/hdd/rc_fq/summer23/july6_bad_rupts.npy', np.array(bad_rupts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a5a0f",
   "metadata": {},
   "source": [
    "### Format and Split Training, Validation, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2910c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full FakeQuakes data shape: (916010,)\n",
      "FakeQuakes training data shape: (732808,)\n",
      "FakeQuakes validation data shape: (91601,)\n",
      "FakeQuakes testing data shape: (91601,)\n",
      "Full noise data shape: (916010,)\n",
      "Noise training data shape: (732808,)\n",
      "Noise validation data shape: (91601,)\n",
      "Noise testing data shape: (91601,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(27)\n",
    "\n",
    "# Earthquake/signal data\n",
    "fqinds = np.arange(fq_data.shape[0]) # Signal indices\n",
    "np.random.shuffle(fqinds) # Shuffles the indices\n",
    "train_fqinds = fqinds[:int(0.8*len(fqinds))] # Training data separation: grabs the front 80% of the numbers\n",
    "valid_fqinds = fqinds[int(0.8*len(fqinds)):int(0.9*len(fqinds)):] # Grabs the next 10% (80-90%)\n",
    "test_fqinds = fqinds[int(0.9*len(fqinds)):] # Grabs the last 10% (90%-end)\n",
    "\n",
    "# Noise data\n",
    "noiseinds = np.arange(noise_data.shape[0]) # Noise indices\n",
    "np.random.shuffle(noiseinds) # Shuffles the indices\n",
    "train_noiseinds = noiseinds[:int(0.8*len(noiseinds))] # Data separation as above\n",
    "valid_noiseinds = noiseinds[int(0.8*len(noiseinds)):int(0.9*len(noiseinds))]\n",
    "test_noiseinds = noiseinds[int(0.9*len(noiseinds)):]\n",
    "\n",
    "# Check shapes to confirm compatability\n",
    "print('Full FakeQuakes data shape: ' + str(fqinds.shape))\n",
    "print('FakeQuakes training data shape: ' + str(train_fqinds.shape))\n",
    "print('FakeQuakes validation data shape: ' + str(valid_fqinds.shape))\n",
    "print('FakeQuakes testing data shape: ' + str(test_fqinds.shape))\n",
    "print('Full noise data shape: ' + str(noiseinds.shape))\n",
    "print('Noise training data shape: ' + str(train_noiseinds.shape))\n",
    "print('Noise validation data shape: ' + str(valid_noiseinds.shape))\n",
    "print('Noise testing data shape: ' + str(test_noiseinds.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce024b",
   "metadata": {},
   "source": [
    "### Check Loaded Data with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba1d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the FakeQuakes data\n",
    "plt.figure(figsize = (8,5))   \n",
    "plt.title('Earthquake test', fontsize = 14)\n",
    "for idx in range(10): # plot 10 of them\n",
    "    plt.plot(fq_data[idx,:] / np.max(np.abs(fq_data[idx,:])) + idx) # Normalized and offset for each idx\n",
    "plt.axvline(256.5, linestyle = '--', color = 'lightgray')\n",
    "plt.axvline(513.5, linestyle = '--', color = 'lightgray')\n",
    "plt.xlabel('Time (s)', fontsize = 12)\n",
    "plt.ylabel('Normalized amplitude', fontsize = 12)\n",
    "plt.xlim(0,770)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(x = 5, y = -0.75, s = 'N', fontsize = 20)\n",
    "plt.text(x = 261, y = -0.75, s = 'E', fontsize = 20)\n",
    "plt.text(x = 518, y = -0.75, s = 'Z', fontsize = 20)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(base_figure_save_dir + '1_plot_raw_eq_data.png', format = 'PNG')\n",
    "plt.close()\n",
    "\n",
    "# Plot noise to check\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.title('Noise test', fontsize = 14)\n",
    "for idx in range(10):\n",
    "    plt.plot(noise_data[idx,:] / np.max(np.abs(noise_data[idx,:])) + idx)\n",
    "plt.axvline(256.5, linestyle = '--', color = 'lightgray')\n",
    "plt.axvline(513.5, linestyle = '--', color = 'lightgray')\n",
    "plt.xlabel('Time (s)', fontsize = 12)\n",
    "plt.ylabel('Normalized amplitude', fontsize = 12)\n",
    "plt.xlim(0,770)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(x = 5, y = -1.25, s = 'N', fontsize = 20)\n",
    "plt.text(x = 261, y = -1.25, s = 'E', fontsize = 20)\n",
    "plt.text(x = 518, y = -1.25, s = 'Z', fontsize = 20)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(base_figure_save_dir + '2_plot_noise_data.png', format = 'PNG')\n",
    "plt.close()\n",
    "\n",
    "# Check the PGD distribution\n",
    "\n",
    "# testing_data = fq_data[test_fqinds]\n",
    "\n",
    "# pgd = np.zeros(testing_data.shape[0]) # Reminder - FQ data is in meters\n",
    "# for idx in range(testing_data.shape[0]):\n",
    "#     pgd[idx] = np.max(np.sqrt((testing_data[idx,:257])**2 + (testing_data[idx,257:514])**2 + (testing_data[idx,514:])**2))\n",
    "\n",
    "# plt.figure(figsize = (8,5))\n",
    "# plt.hist(pgd, bins = 30, alpha = 0.5, edgecolor = 'black')\n",
    "# plt.ylim(0,10000)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c113ec",
   "metadata": {},
   "source": [
    "## Test of Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b594ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generator check original data shape: (32, 128, 3)\n",
      "Data generator check normalized data shape: (32, 128, 3)\n",
      "Data generator check target shape: (32, 128)\n",
      "Data generator check metadata shape: (32, 3)\n"
     ]
    }
   ],
   "source": [
    "checkgen = X_gnss_unet_datagen_fn19.my_3comp_data_generator(32, fq_data, noise_data, fq_metadata, nan_array, train_fqinds, train_noiseinds, sr, std, valid = True) # Valid = True to get original data back\n",
    "checkgen_orig_data, checkgen_norm_data, checkgen_target, checkgen_metadata = next(checkgen) \n",
    "\n",
    "print('Data generator check original data shape: ' + str(checkgen_orig_data.shape))\n",
    "print('Data generator check normalized data shape: ' + str(checkgen_norm_data.shape))\n",
    "print('Data generator check target shape: ' + str(checkgen_target.shape))\n",
    "print('Data generator check metadata shape: ' + str(checkgen_metadata.shape))\n",
    "\n",
    "# Shapes:\n",
    "    # data: (batch_size, 128, 3) # N, E, Z\n",
    "    # target: (batch_size, 128)\n",
    "    # metadata: (batch_size, 3) Rupt name, station name, magnitude\n",
    "\n",
    "# Plot generator results\n",
    "\n",
    "nexamples = 10 # Number of examples to look at \n",
    "  \n",
    "for ind in range(nexamples): \n",
    "    \n",
    "#     print('Magnitude: ' + str(metadata[ind,2]))\n",
    "\n",
    "    fig = plt.subplots(nrows = 1, ncols = 3, figsize = (26,4), dpi = 300) # shoter for AGU talk\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    t = 1/sr * np.arange(checkgen_orig_data.shape[1])\n",
    "    \n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.plot(t, checkgen_orig_data[ind,:,0]*100, label = 'N original data', color = 'C0')\n",
    "    ax1.plot(t, checkgen_norm_data[ind,:,0]*100, label = 'N normalized data', color = 'C0', linestyle = '--')\n",
    "    ax1.set_ylabel('Displacement (cm)')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, checkgen_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.legend(loc = 'lower right')\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.plot(t, checkgen_orig_data[ind,:,1]*100, label = 'E original data', color = 'C1')\n",
    "    ax3.plot(t, checkgen_norm_data[ind,:,1]*100, label = 'E normalized data', color = 'C1', linestyle = '--')\n",
    "    ax3.set_ylabel('Displacement (cm)')\n",
    "    ax3.legend(loc = 'upper right')\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(t, checkgen_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax4.legend(loc = 'lower right')\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(t, checkgen_orig_data[ind,:,2]*100, label = 'Z original data', color = 'C2')\n",
    "    ax5.plot(t, checkgen_norm_data[ind,:,2]*100, label = 'Z normalized data', color = 'C2', linestyle = '--')\n",
    "    ax5.set_ylabel('Displacement (cm)')\n",
    "    ax5.legend(loc = 'upper right')\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(t, checkgen_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax6.legend(loc = 'lower right')\n",
    "    \n",
    "#     plt.show()\n",
    "    plt.savefig(base_figure_save_dir + '3_ex' + str(ind) + '_plot_generator_pass.png', format = 'PNG')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b12f7",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b91607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 128, 16)      1024        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 64, 16)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 64, 32)       7712        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 32, 32)      0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 32, 64)       22592       ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 16, 64)      0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1024)         0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           16400       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 16, 1)        0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 16, 64)       768         ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 32, 64)       0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 128)      0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32, 32)       61472       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 64, 32)      0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64)       0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 64, 16)       21520       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling1d_2 (UpSampling1D)  (None, 128, 16)     0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 32)      0           ['up_sampling1d_2[0][0]',        \n",
      "                                                                  'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 128, 1)       673         ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 128)          0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,161\n",
      "Trainable params: 132,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Using model with dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 17:14:47.504395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504489: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504562: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504718: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-27 17:14:47.504759: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-27 17:14:47.505812: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sdybing/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "if drop: # Use a model with a dropout layer\n",
    "    model = X_gnss_unet_datagen_fn19.make_large_unet_drop(fac, sr, ncomps = 3)\n",
    "    print('Using model with dropout')\n",
    "else:\n",
    "    model = X_gnss_unet_datagen_fn19.make_large_unet(fac, sr, ncomps = 3)  \n",
    "    print('Using large model')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aff619",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a6c69",
   "metadata": {},
   "source": [
    "### See how training works with a smaller dataset (faster)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffea016",
   "metadata": {},
   "outputs": [],
   "source": [
    "if small_train:\n",
    "    train_fqinds = train_fqinds[:10000]\n",
    "    train_noiseinds = train_noiseinds[:10000]\n",
    "    valid_fqinds = valid_fqinds[:10000]\n",
    "    valid_noiseinds = valid_noiseinds[:10000]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915f6a0",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33537b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    \n",
    "    traindate = date.today()\n",
    "    \n",
    "    model_save_dir = models_path + 'traindate_' + str(traindate) + '/' # Where to save the trained model\n",
    "    data_save_dir = model_save_dir + 'data/' # Where to save the outputted testing data and predictions\n",
    "    figure_save_dir = model_save_dir + 'figures/' # Where to save the figures\n",
    "    \n",
    "    if os.path.isdir(model_save_dir):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(model_save_dir)\n",
    "        os.makedirs(data_save_dir)\n",
    "        os.makedirs(figure_save_dir)\n",
    "    \n",
    "    model_save_file = model_save_dir + 'bestmodel_traindate_' + str(traindate) + '.h5'\n",
    "    \n",
    "    print('Training model and saving results to ' + model_save_file)\n",
    "    \n",
    "    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown = 0, patience = 4, min_lr = 0.5e-6)\n",
    "    early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = model_save_file, monitor = 'val_loss', mode = 'auto', verbose = 1, save_best_only = True)\n",
    "    callbacks = [lr_reducer, early_stopping_monitor, checkpoint]\n",
    "    \n",
    "    history = model.fit(X_gnss_unet_datagen_fn19.my_3comp_data_generator(batch_size, fq_data, noise_data, fq_metadata, nan_array, train_fqinds, train_noiseinds, sr, std), # Valid = False for training; implied\n",
    "                        steps_per_epoch = (len(train_fqinds) + len(train_noiseinds))//batch_size,\n",
    "                        validation_data = X_gnss_unet_datagen_fn19.my_3comp_data_generator(batch_size, fq_data, noise_data, fq_metadata, nan_array, valid_fqinds, valid_noiseinds, sr, std),\n",
    "                        validation_steps = (len(valid_fqinds) + len(valid_noiseinds))//batch_size,\n",
    "                        epochs = epochs, callbacks = callbacks)\n",
    "    \n",
    "    model.save_weights(model_save_file)\n",
    "    np.save(model_save_dir + 'training_history.npy', history.history)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1ea2d",
   "metadata": {},
   "source": [
    "### Check Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd64ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    \n",
    "    history = np.load(model_save_dir + 'training_history.npy', allow_pickle = 'TRUE').item()\n",
    "\n",
    "    fig = plt.subplots(nrows = 2, ncols = 1, figsize = (6,8))\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.plot(history['loss'], label = 'Training loss')\n",
    "    ax1.plot(history['val_loss'], label = 'Validation loss') \n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Model: traindate_' + str(traindate) + '.h5')\n",
    "\n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.plot(history['accuracy'], label = 'Training accuracy') \n",
    "    ax2.plot(history['val_accuracy'], label = 'Validation accuracy') \n",
    "    ax2.legend(loc = 'lower right')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    plt.subplots_adjust(hspace = 0)\n",
    "\n",
    "#     plt.show()\n",
    "    plt.savefig(figure_save_dir + '4_training_curves.png', format = 'PNG')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d756c8d",
   "metadata": {},
   "source": [
    "## Load an old trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2358f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training results from /home/sdybing/gnss-picker/cnn_models_outputs/july6_fq_train/models/traindate_2023-08-26/bestmodel_traindate_2023-08-26.h5\n"
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    loaddate = '2023-08-26' # Format: YYYY-MM-DD\n",
    "    model_load_file = project_save_dir + 'models/traindate_' + str(loaddate) + '/bestmodel_traindate_' + str(loaddate) + '.h5'\n",
    "    data_save_dir = models_path + 'traindate_' + str(loaddate) + '/data/' # Where to save the outputted testing data and predictions\n",
    "    figure_save_dir = models_path + 'traindate_' + str(loaddate) + '/figures/'\n",
    "    print('Loading training results from ' + model_load_file)\n",
    "    \n",
    "    model.load_weights(model_load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6eadb",
   "metadata": {},
   "source": [
    "## Test the Model with Remaining FakeQuakes Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642828f8",
   "metadata": {},
   "source": [
    "### See how testing works with a smaller dataset (faster)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ca4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if small_test:\n",
    "    test_fqinds = test_fqinds[:100]\n",
    "    test_noiseinds = test_noiseinds[:100]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a838dd",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "731f4e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "FQ test original data shape: (91598, 128, 3)\n",
      "FQ test normalized data shape: (91598, 128, 3)\n",
      "FQ test metadata shape: (91598, 3)\n",
      "FQ test target shape: (91598, 128)\n",
      "FQ test predictions shape: (91598, 128)\n"
     ]
    }
   ],
   "source": [
    "fqtestdate = date.today()\n",
    "num_fqtest = len(test_fqinds) - 3 # Number of samples to test with\n",
    "# print(num_fqtest)\n",
    "\n",
    "fqtestmodel = X_gnss_unet_datagen_fn19.my_3comp_data_generator(num_fqtest, fq_data, noise_data, fq_metadata, nan_array, test_fqinds, test_noiseinds, sr, std, valid = True)\n",
    "fqtest_orig_data, fqtest_norm_data, fqtest_target, fqtest_metadata = next(fqtestmodel)\n",
    "print('Predicting...')\n",
    "fqtest_predictions = model.predict(fqtest_norm_data)\n",
    "\n",
    "print('FQ test original data shape: ' + str(fqtest_orig_data.shape))\n",
    "print('FQ test normalized data shape: ' + str(fqtest_norm_data.shape))\n",
    "print('FQ test metadata shape: ' + str(fqtest_metadata.shape))\n",
    "print('FQ test target shape: ' + str(fqtest_target.shape))\n",
    "print('FQ test predictions shape: ' + str(fqtest_predictions.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f45e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing with validation data\n",
    "\n",
    "# fqtestdate = date.today()\n",
    "# num_fqtest = len(valid_fqinds) - 2 # Number of samples to test with\n",
    "# # print(num_fqtest)\n",
    "\n",
    "# fqtestmodel = X_gnss_unet_datagen_fn9.my_3comp_data_generator(num_fqtest, fq_data, noise_data, fq_metadata, nan_array, valid_fqinds, valid_noiseinds, sr, std, valid = True)\n",
    "# fqtest_data, fqtest_target, fqtest_metadata = next(fqtestmodel)\n",
    "# print('Predicting...')\n",
    "# fqtest_predictions = model.predict(fqtest_data)\n",
    "\n",
    "# print('FQ test data shape: ' + str(fqtest_data.shape))\n",
    "# print('FQ test metadata shape: ' + str(fqtest_metadata.shape))\n",
    "# print('FQ test target shape: ' + str(fqtest_target.shape))\n",
    "# print('FQ test predictions shape: ' + str(fqtest_predictions.shape))\n",
    "\n",
    "# np.save(data_save_dir + str(fqtestdate) + '_fqvalid_data.npy', fqtest_data)\n",
    "# np.save(data_save_dir + str(fqtestdate) + '_fqvalid_metadata.npy', fqtest_metadata)\n",
    "# np.save(data_save_dir + str(fqtestdate) + '_fqvalid_target.npy', fqtest_target)\n",
    "# np.save(data_save_dir + str(fqtestdate) + '_fqvalid_predictions.npy', fqtest_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc4ad6",
   "metadata": {},
   "source": [
    "### Save the FQ testing data, targets, metadata, and predictions as .npys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46376868",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_orig_data.npy', fqtest_orig_data)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_norm_data.npy', fqtest_norm_data)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_metadata.npy', fqtest_metadata)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_target.npy', fqtest_target)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_predictions.npy', fqtest_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceba3e5",
   "metadata": {},
   "source": [
    "### Check PGD distribution of FQ testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f96efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fqtest_pgd = np.zeros(fqtest_orig_data.shape[0])\n",
    "for idx in range(fqtest_orig_data.shape[0]):\n",
    "    fqtest_pgd[idx] = np.max(np.sqrt((fqtest_orig_data[idx,:,0])**2 + (fqtest_orig_data[idx,:,1])**2 + (fqtest_orig_data[idx,:,2])**2))\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.hist(np.log10(fqtest_pgd), bins = 30, alpha = 0.5, edgecolor = 'black')\n",
    "# plt.ylim(0,4000)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(figure_save_dir + '5a_fqtestdata_pgd_distrib.png', format = 'PNG')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff594f2",
   "metadata": {},
   "source": [
    "### Plot checks of FQ testing data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e81e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nexamples = 5 # Number of examples to look at \n",
    "  \n",
    "for ind in range(nexamples): \n",
    "    \n",
    "    fig = plt.subplots(nrows = 1, ncols = 3, figsize = (18,4), dpi = 300)\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    t = 1/sr * np.arange(fqtest_orig_data.shape[1])\n",
    "    # print(t)\n",
    "    \n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.plot(t, fqtest_norm_data[ind,:,0], label = 'N test data', color = 'C0')\n",
    "    ax1.set_ylabel('Displacement (m)')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, fqtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax2.plot(t, fqtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.set_ylim(-0.05,1.05)\n",
    "    ax2.legend(loc = 'upper left')\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.plot(t, fqtest_norm_data[ind,:,1], label = 'E test data', color = 'C1')\n",
    "    ax3.set_ylabel('Displacement (m)')\n",
    "    ax3.legend(loc = 'upper right')\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(t, fqtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax4.plot(t, fqtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax4.set_ylim(-0.05,1.05)\n",
    "    ax4.legend(loc = 'upper left')\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(t, fqtest_norm_data[ind,:,2], label = 'Z test data', color = 'C2')\n",
    "    ax5.set_ylabel('Displacement (m)')\n",
    "    ax5.legend(loc = 'upper right')\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(t, fqtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax6.plot(t, fqtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax6.set_ylim(-0.05,1.05)\n",
    "    ax6.legend(loc = 'upper left')\n",
    "    \n",
    "#     plt.show()\n",
    "    plt.savefig(figure_save_dir + '6_fqtestdata_ex' + str(ind) + '_plot_test_predictions.png', format = 'PNG')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337289e",
   "metadata": {},
   "source": [
    "## Test the Model with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778674e4",
   "metadata": {},
   "source": [
    "### Run Real Data through Data Generator and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8c0dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Real test original data shape: (994155, 128, 3)\n",
      "Real test normalize data shape: (994155, 128, 3)\n",
      "Real test metadata shape: (994155, 7)\n",
      "Real test target shape: (994155, 128)\n",
      "Real test predictions shape: (994155, 128)\n"
     ]
    }
   ],
   "source": [
    "realtestdate = date.today()\n",
    "\n",
    "if small_test:\n",
    "    realtest_data = real_data[:100]\n",
    "    realtest_metadata = real_metadata[:100]\n",
    "    num_realtest = len(realtest_data)\n",
    "    realtestmodel = X_gnss_unet_datagen_fn22.real_data_generator(batch_size = num_realtest, data = realtest_data, meta_data = realtest_metadata, sr = 1, std = 3, nlen = 128)\n",
    "    realtest_orig_data, realtest_norm_data, realtest_target = next(realtestmodel)\n",
    "    \n",
    "else:\n",
    "    num_realtest = len(real_data) # Number of samples to test with\n",
    "    realtestmodel = X_gnss_unet_datagen_fn22.real_data_generator(batch_size = num_realtest, data = real_data, meta_data = real_metadata, sr = 1, std = 3, nlen = 128)\n",
    "    realtest_orig_data, realtest_norm_data, realtest_target = next(realtestmodel)\n",
    "    realtest_metadata = real_metadata\n",
    "\n",
    "print('Predicting...')\n",
    "realtest_predictions = model.predict(realtest_norm_data)\n",
    "\n",
    "print('Real test original data shape: ' + str(realtest_orig_data.shape))\n",
    "print('Real test normalize data shape: ' + str(realtest_norm_data.shape))\n",
    "print('Real test metadata shape: ' + str(realtest_metadata.shape))\n",
    "print('Real test target shape: ' + str(realtest_target.shape))\n",
    "print('Real test predictions shape: ' + str(realtest_predictions.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcb465",
   "metadata": {},
   "source": [
    "### Save the real testing data, targets, metadata, and predictions as .npys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9801cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_save_dir + str(realtestdate) + '_realtest_orig_data.npy', realtest_orig_data)\n",
    "np.save(data_save_dir + str(realtestdate) + '_realtest_norm_data.npy', realtest_norm_data)\n",
    "np.save(data_save_dir + str(realtestdate) + '_realtest_metadata.npy', realtest_metadata)\n",
    "np.save(data_save_dir + str(realtestdate) + '_realtest_target.npy', realtest_target)\n",
    "np.save(data_save_dir + str(realtestdate) + '_realtest_predictions.npy', realtest_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c3535",
   "metadata": {},
   "source": [
    "### Check PGD distribution of real testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9a023eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.245717481790863"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realtest_pgd = np.zeros(realtest_orig_data.shape[0])\n",
    "for idx in range(realtest_orig_data.shape[0]):\n",
    "    realtest_pgd[idx] = np.max(np.sqrt((realtest_orig_data[idx,:,0])**2 + (realtest_orig_data[idx,:,1])**2 + (realtest_orig_data[idx,:,2])**2))\n",
    "\n",
    "max(realtest_pgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99271bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.hist(np.log10(realtest_pgd), bins = 30, alpha = 0.5, edgecolor = 'black')\n",
    "plt.xlabel('Log PGD (m)')\n",
    "plt.ylabel('Count')\n",
    "# plt.ylim(0,1000)\n",
    "\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '15_realtestdata_pgd_distrib.png', format = 'PNG')\n",
    "plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a222cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1473   4079  15197 ... 992695 992821 993524]\n",
      "[['ACSB' '20190706' '2019-07-06T04:37:17.000000Z' ... '131' '21' '4.85']\n",
      " ['ACSB' '20190726' '2019-07-26T00:42:20.000000Z' ... '20' '73' '4.74']\n",
      " ['BBRY' '20190704' '2019-07-04T17:34:06.000000Z' ... '498' '10' '6.40']\n",
      " ...\n",
      " ['WKPK' '20190716' '2019-07-16T20:14:58.000000Z' ... '574' '66' '4.47']\n",
      " ['WKPK' '20190726' '2019-07-26T00:42:20.000000Z' ... '20' '57' '4.74']\n",
      " ['WKPK' '20200604' '2020-06-04T01:31:01.000000Z' ... '43' '97' '5.51']]\n"
     ]
    }
   ],
   "source": [
    "rows_w_eqs = np.load(realdata_dir + 'real_metadata_rembad_rows_w_eqs.npy')\n",
    "print(rows_w_eqs)\n",
    "\n",
    "print(realtest_metadata[rows_w_eqs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b287c",
   "metadata": {},
   "source": [
    "### Plot checks of real testing data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a083b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real earthquake 1/2123\n",
      "Real earthquake 2/2123\n",
      "Real earthquake 3/2123\n",
      "Real earthquake 4/2123\n",
      "Real earthquake 5/2123\n",
      "Real earthquake 6/2123\n",
      "Real earthquake 7/2123\n",
      "Real earthquake 8/2123\n",
      "Real earthquake 9/2123\n",
      "Real earthquake 10/2123\n",
      "Real earthquake 11/2123\n",
      "Real earthquake 12/2123\n",
      "Real earthquake 13/2123\n",
      "Real earthquake 14/2123\n",
      "Real earthquake 15/2123\n",
      "Real earthquake 16/2123\n",
      "Real earthquake 17/2123\n",
      "Real earthquake 18/2123\n",
      "Real earthquake 19/2123\n",
      "Real earthquake 20/2123\n",
      "Real earthquake 21/2123\n",
      "Real earthquake 22/2123\n",
      "Real earthquake 23/2123\n",
      "Real earthquake 24/2123\n",
      "Real earthquake 25/2123\n",
      "Real earthquake 26/2123\n",
      "Real earthquake 27/2123\n",
      "Real earthquake 28/2123\n",
      "Real earthquake 29/2123\n",
      "Real earthquake 30/2123\n",
      "Real earthquake 31/2123\n",
      "Real earthquake 32/2123\n",
      "Real earthquake 33/2123\n",
      "Real earthquake 34/2123\n",
      "Real earthquake 35/2123\n",
      "Real earthquake 36/2123\n",
      "Real earthquake 37/2123\n",
      "Real earthquake 38/2123\n",
      "Real earthquake 39/2123\n",
      "Real earthquake 40/2123\n",
      "Real earthquake 41/2123\n",
      "Real earthquake 42/2123\n",
      "Real earthquake 43/2123\n",
      "Real earthquake 44/2123\n",
      "Real earthquake 45/2123\n",
      "Real earthquake 46/2123\n",
      "Real earthquake 47/2123\n",
      "Real earthquake 48/2123\n",
      "Real earthquake 49/2123\n",
      "Real earthquake 50/2123\n",
      "Real earthquake 51/2123\n",
      "Real earthquake 52/2123\n",
      "Real earthquake 53/2123\n",
      "Real earthquake 54/2123\n",
      "Real earthquake 55/2123\n",
      "Real earthquake 56/2123\n",
      "Real earthquake 57/2123\n",
      "Real earthquake 58/2123\n",
      "Real earthquake 59/2123\n",
      "Real earthquake 60/2123\n",
      "Real earthquake 61/2123\n",
      "Real earthquake 62/2123\n",
      "Real earthquake 63/2123\n",
      "Real earthquake 64/2123\n",
      "Real earthquake 65/2123\n",
      "Real earthquake 66/2123\n",
      "Real earthquake 67/2123\n",
      "Real earthquake 68/2123\n",
      "Real earthquake 69/2123\n",
      "Real earthquake 70/2123\n",
      "Real earthquake 71/2123\n",
      "Real earthquake 72/2123\n",
      "Real earthquake 73/2123\n",
      "Real earthquake 74/2123\n",
      "Real earthquake 75/2123\n",
      "Real earthquake 76/2123\n",
      "Real earthquake 77/2123\n",
      "Real earthquake 78/2123\n",
      "Real earthquake 79/2123\n",
      "Real earthquake 80/2123\n",
      "Real earthquake 81/2123\n",
      "Real earthquake 82/2123\n",
      "Real earthquake 83/2123\n",
      "Real earthquake 84/2123\n",
      "Real earthquake 85/2123\n",
      "Real earthquake 86/2123\n",
      "Real earthquake 87/2123\n",
      "Real earthquake 88/2123\n",
      "Real earthquake 89/2123\n",
      "Real earthquake 90/2123\n",
      "Real earthquake 91/2123\n",
      "Real earthquake 92/2123\n",
      "Real earthquake 93/2123\n",
      "Real earthquake 94/2123\n",
      "Real earthquake 95/2123\n",
      "Real earthquake 96/2123\n",
      "Real earthquake 97/2123\n",
      "Real earthquake 98/2123\n",
      "Real earthquake 99/2123\n",
      "Real earthquake 100/2123\n",
      "Real earthquake 101/2123\n",
      "Real earthquake 102/2123\n",
      "Real earthquake 103/2123\n",
      "Real earthquake 104/2123\n",
      "Real earthquake 105/2123\n",
      "Real earthquake 106/2123\n",
      "Real earthquake 107/2123\n",
      "Real earthquake 108/2123\n",
      "Real earthquake 109/2123\n",
      "Real earthquake 110/2123\n",
      "Real earthquake 111/2123\n",
      "Real earthquake 112/2123\n",
      "Real earthquake 113/2123\n",
      "Real earthquake 114/2123\n",
      "Real earthquake 115/2123\n",
      "Real earthquake 116/2123\n",
      "Real earthquake 117/2123\n",
      "Real earthquake 118/2123\n",
      "Real earthquake 119/2123\n",
      "Real earthquake 120/2123\n",
      "Real earthquake 121/2123\n",
      "Real earthquake 122/2123\n",
      "Real earthquake 123/2123\n",
      "Real earthquake 124/2123\n",
      "Real earthquake 125/2123\n",
      "Real earthquake 126/2123\n",
      "Real earthquake 127/2123\n",
      "Real earthquake 128/2123\n",
      "Real earthquake 129/2123\n",
      "Real earthquake 130/2123\n",
      "Real earthquake 131/2123\n",
      "Real earthquake 132/2123\n",
      "Real earthquake 133/2123\n",
      "Real earthquake 134/2123\n",
      "Real earthquake 135/2123\n",
      "Real earthquake 136/2123\n",
      "Real earthquake 137/2123\n",
      "Real earthquake 138/2123\n",
      "Real earthquake 139/2123\n",
      "Real earthquake 140/2123\n",
      "Real earthquake 141/2123\n",
      "Real earthquake 142/2123\n",
      "Real earthquake 143/2123\n",
      "Real earthquake 144/2123\n",
      "Real earthquake 145/2123\n",
      "Real earthquake 146/2123\n",
      "Real earthquake 147/2123\n",
      "Real earthquake 148/2123\n",
      "Real earthquake 149/2123\n",
      "Real earthquake 150/2123\n",
      "Real earthquake 151/2123\n",
      "Real earthquake 152/2123\n",
      "Real earthquake 153/2123\n",
      "Real earthquake 154/2123\n",
      "Real earthquake 155/2123\n",
      "Real earthquake 156/2123\n",
      "Real earthquake 157/2123\n",
      "Real earthquake 158/2123\n",
      "Real earthquake 159/2123\n",
      "Real earthquake 160/2123\n",
      "Real earthquake 161/2123\n",
      "Real earthquake 162/2123\n",
      "Real earthquake 163/2123\n",
      "Real earthquake 164/2123\n",
      "Real earthquake 165/2123\n",
      "Real earthquake 166/2123\n",
      "Real earthquake 167/2123\n",
      "Real earthquake 168/2123\n",
      "Real earthquake 169/2123\n",
      "Real earthquake 170/2123\n",
      "Real earthquake 171/2123\n",
      "Real earthquake 172/2123\n",
      "Real earthquake 173/2123\n",
      "Real earthquake 174/2123\n",
      "Real earthquake 175/2123\n",
      "Real earthquake 176/2123\n",
      "Real earthquake 177/2123\n",
      "Real earthquake 178/2123\n",
      "Real earthquake 179/2123\n",
      "Real earthquake 180/2123\n",
      "Real earthquake 181/2123\n",
      "Real earthquake 182/2123\n",
      "Real earthquake 183/2123\n",
      "Real earthquake 184/2123\n",
      "Real earthquake 185/2123\n",
      "Real earthquake 186/2123\n",
      "Real earthquake 187/2123\n",
      "Real earthquake 188/2123\n",
      "Real earthquake 189/2123\n",
      "Real earthquake 190/2123\n",
      "Real earthquake 191/2123\n",
      "Real earthquake 192/2123\n",
      "Real earthquake 193/2123\n",
      "Real earthquake 194/2123\n",
      "Real earthquake 195/2123\n",
      "Real earthquake 196/2123\n",
      "Real earthquake 197/2123\n",
      "Real earthquake 198/2123\n",
      "Real earthquake 199/2123\n",
      "Real earthquake 200/2123\n",
      "Real earthquake 201/2123\n",
      "Real earthquake 202/2123\n",
      "Real earthquake 203/2123\n",
      "Real earthquake 204/2123\n",
      "Real earthquake 205/2123\n",
      "Real earthquake 206/2123\n",
      "Real earthquake 207/2123\n",
      "Real earthquake 208/2123\n",
      "Real earthquake 209/2123\n",
      "Real earthquake 210/2123\n",
      "Real earthquake 211/2123\n",
      "Real earthquake 212/2123\n",
      "Real earthquake 213/2123\n",
      "Real earthquake 214/2123\n",
      "Real earthquake 215/2123\n",
      "Real earthquake 216/2123\n",
      "Real earthquake 217/2123\n",
      "Real earthquake 218/2123\n",
      "Real earthquake 219/2123\n",
      "Real earthquake 220/2123\n",
      "Real earthquake 221/2123\n",
      "Real earthquake 222/2123\n",
      "Real earthquake 223/2123\n",
      "Real earthquake 224/2123\n",
      "Real earthquake 225/2123\n",
      "Real earthquake 226/2123\n",
      "Real earthquake 227/2123\n",
      "Real earthquake 228/2123\n",
      "Real earthquake 229/2123\n",
      "Real earthquake 230/2123\n",
      "Real earthquake 231/2123\n",
      "Real earthquake 232/2123\n",
      "Real earthquake 233/2123\n",
      "Real earthquake 234/2123\n",
      "Real earthquake 235/2123\n",
      "Real earthquake 236/2123\n",
      "Real earthquake 237/2123\n",
      "Real earthquake 238/2123\n",
      "Real earthquake 239/2123\n",
      "Real earthquake 240/2123\n",
      "Real earthquake 241/2123\n",
      "Real earthquake 242/2123\n",
      "Real earthquake 243/2123\n",
      "Real earthquake 244/2123\n",
      "Real earthquake 245/2123\n",
      "Real earthquake 246/2123\n",
      "Real earthquake 247/2123\n",
      "Real earthquake 248/2123\n",
      "Real earthquake 249/2123\n",
      "Real earthquake 250/2123\n",
      "Real earthquake 251/2123\n",
      "Real earthquake 252/2123\n",
      "Real earthquake 253/2123\n",
      "Real earthquake 254/2123\n",
      "Real earthquake 255/2123\n",
      "Real earthquake 256/2123\n",
      "Real earthquake 257/2123\n",
      "Real earthquake 258/2123\n",
      "Real earthquake 259/2123\n",
      "Real earthquake 260/2123\n",
      "Real earthquake 261/2123\n",
      "Real earthquake 262/2123\n",
      "Real earthquake 263/2123\n",
      "Real earthquake 264/2123\n",
      "Real earthquake 265/2123\n",
      "Real earthquake 266/2123\n",
      "Real earthquake 267/2123\n",
      "Real earthquake 268/2123\n",
      "Real earthquake 269/2123\n",
      "Real earthquake 270/2123\n",
      "Real earthquake 271/2123\n",
      "Real earthquake 272/2123\n",
      "Real earthquake 273/2123\n",
      "Real earthquake 274/2123\n",
      "Real earthquake 275/2123\n",
      "Real earthquake 276/2123\n",
      "Real earthquake 277/2123\n",
      "Real earthquake 278/2123\n",
      "Real earthquake 279/2123\n",
      "Real earthquake 280/2123\n",
      "Real earthquake 281/2123\n",
      "Real earthquake 282/2123\n",
      "Real earthquake 283/2123\n",
      "Real earthquake 284/2123\n",
      "Real earthquake 285/2123\n",
      "Real earthquake 286/2123\n",
      "Real earthquake 287/2123\n",
      "Real earthquake 288/2123\n",
      "Real earthquake 289/2123\n",
      "Real earthquake 290/2123\n",
      "Real earthquake 291/2123\n",
      "Real earthquake 292/2123\n",
      "Real earthquake 293/2123\n",
      "Real earthquake 294/2123\n",
      "Real earthquake 295/2123\n",
      "Real earthquake 296/2123\n",
      "Real earthquake 297/2123\n",
      "Real earthquake 298/2123\n",
      "Real earthquake 299/2123\n",
      "Real earthquake 300/2123\n",
      "Real earthquake 301/2123\n",
      "Real earthquake 302/2123\n",
      "Real earthquake 303/2123\n",
      "Real earthquake 304/2123\n",
      "Real earthquake 305/2123\n",
      "Real earthquake 306/2123\n",
      "Real earthquake 307/2123\n",
      "Real earthquake 308/2123\n",
      "Real earthquake 309/2123\n",
      "Real earthquake 310/2123\n",
      "Real earthquake 311/2123\n",
      "Real earthquake 312/2123\n",
      "Real earthquake 313/2123\n",
      "Real earthquake 314/2123\n",
      "Real earthquake 315/2123\n",
      "Real earthquake 316/2123\n",
      "Real earthquake 317/2123\n",
      "Real earthquake 318/2123\n",
      "Real earthquake 319/2123\n",
      "Real earthquake 320/2123\n",
      "Real earthquake 321/2123\n",
      "Real earthquake 322/2123\n",
      "Real earthquake 323/2123\n",
      "Real earthquake 324/2123\n",
      "Real earthquake 325/2123\n",
      "Real earthquake 326/2123\n",
      "Real earthquake 327/2123\n",
      "Real earthquake 328/2123\n",
      "Real earthquake 329/2123\n",
      "Real earthquake 330/2123\n",
      "Real earthquake 331/2123\n",
      "Real earthquake 332/2123\n",
      "Real earthquake 333/2123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real earthquake 334/2123\n",
      "Real earthquake 335/2123\n",
      "Real earthquake 336/2123\n",
      "Real earthquake 337/2123\n",
      "Real earthquake 338/2123\n",
      "Real earthquake 339/2123\n",
      "Real earthquake 340/2123\n",
      "Real earthquake 341/2123\n",
      "Real earthquake 342/2123\n",
      "Real earthquake 343/2123\n",
      "Real earthquake 344/2123\n",
      "Real earthquake 345/2123\n",
      "Real earthquake 346/2123\n",
      "Real earthquake 347/2123\n",
      "Real earthquake 348/2123\n",
      "Real earthquake 349/2123\n",
      "Real earthquake 350/2123\n",
      "Real earthquake 351/2123\n",
      "Real earthquake 352/2123\n",
      "Real earthquake 353/2123\n",
      "Real earthquake 354/2123\n",
      "Real earthquake 355/2123\n",
      "Real earthquake 356/2123\n",
      "Real earthquake 357/2123\n",
      "Real earthquake 358/2123\n",
      "Real earthquake 359/2123\n",
      "Real earthquake 360/2123\n",
      "Real earthquake 361/2123\n",
      "Real earthquake 362/2123\n",
      "Real earthquake 363/2123\n",
      "Real earthquake 364/2123\n",
      "Real earthquake 365/2123\n",
      "Real earthquake 366/2123\n",
      "Real earthquake 367/2123\n",
      "Real earthquake 368/2123\n",
      "Real earthquake 369/2123\n",
      "Real earthquake 370/2123\n",
      "Real earthquake 371/2123\n",
      "Real earthquake 372/2123\n",
      "Real earthquake 373/2123\n",
      "Real earthquake 374/2123\n",
      "Real earthquake 375/2123\n",
      "Real earthquake 376/2123\n",
      "Real earthquake 377/2123\n",
      "Real earthquake 378/2123\n",
      "Real earthquake 379/2123\n",
      "Real earthquake 380/2123\n",
      "Real earthquake 381/2123\n",
      "Real earthquake 382/2123\n",
      "Real earthquake 383/2123\n",
      "Real earthquake 384/2123\n",
      "Real earthquake 385/2123\n",
      "Real earthquake 386/2123\n",
      "Real earthquake 387/2123\n",
      "Real earthquake 388/2123\n",
      "Real earthquake 389/2123\n",
      "Real earthquake 390/2123\n",
      "Real earthquake 391/2123\n",
      "Real earthquake 392/2123\n",
      "Real earthquake 393/2123\n",
      "Real earthquake 394/2123\n",
      "Real earthquake 395/2123\n",
      "Real earthquake 396/2123\n",
      "Real earthquake 397/2123\n",
      "Real earthquake 398/2123\n",
      "Real earthquake 399/2123\n",
      "Real earthquake 400/2123\n",
      "Real earthquake 401/2123\n",
      "Real earthquake 402/2123\n",
      "Real earthquake 403/2123\n",
      "Real earthquake 404/2123\n",
      "Real earthquake 405/2123\n",
      "Real earthquake 406/2123\n",
      "Real earthquake 407/2123\n",
      "Real earthquake 408/2123\n",
      "Real earthquake 409/2123\n",
      "Real earthquake 410/2123\n",
      "Real earthquake 411/2123\n",
      "Real earthquake 412/2123\n",
      "Real earthquake 413/2123\n",
      "Real earthquake 414/2123\n",
      "Real earthquake 415/2123\n",
      "Real earthquake 416/2123\n",
      "Real earthquake 417/2123\n",
      "Real earthquake 418/2123\n",
      "Real earthquake 419/2123\n",
      "Real earthquake 420/2123\n",
      "Real earthquake 421/2123\n",
      "Real earthquake 422/2123\n",
      "Real earthquake 423/2123\n",
      "Real earthquake 424/2123\n",
      "Real earthquake 425/2123\n",
      "Real earthquake 426/2123\n",
      "Real earthquake 427/2123\n",
      "Real earthquake 428/2123\n",
      "Real earthquake 429/2123\n",
      "Real earthquake 430/2123\n",
      "Real earthquake 431/2123\n",
      "Real earthquake 432/2123\n",
      "Real earthquake 433/2123\n",
      "Real earthquake 434/2123\n",
      "Real earthquake 435/2123\n",
      "Real earthquake 436/2123\n",
      "Real earthquake 437/2123\n",
      "Real earthquake 438/2123\n",
      "Real earthquake 439/2123\n",
      "Real earthquake 440/2123\n",
      "Real earthquake 441/2123\n",
      "Real earthquake 442/2123\n",
      "Real earthquake 443/2123\n",
      "Real earthquake 444/2123\n",
      "Real earthquake 445/2123\n",
      "Real earthquake 446/2123\n",
      "Real earthquake 447/2123\n",
      "Real earthquake 448/2123\n",
      "Real earthquake 449/2123\n",
      "Real earthquake 450/2123\n",
      "Real earthquake 451/2123\n",
      "Real earthquake 452/2123\n",
      "Real earthquake 453/2123\n",
      "Real earthquake 454/2123\n",
      "Real earthquake 455/2123\n",
      "Real earthquake 456/2123\n",
      "Real earthquake 457/2123\n",
      "Real earthquake 458/2123\n",
      "Real earthquake 459/2123\n",
      "Real earthquake 460/2123\n",
      "Real earthquake 461/2123\n",
      "Real earthquake 462/2123\n",
      "Real earthquake 463/2123\n",
      "Real earthquake 464/2123\n",
      "Real earthquake 465/2123\n",
      "Real earthquake 466/2123\n",
      "Real earthquake 467/2123\n",
      "Real earthquake 468/2123\n",
      "Real earthquake 469/2123\n",
      "Real earthquake 470/2123\n",
      "Real earthquake 471/2123\n",
      "Real earthquake 472/2123\n",
      "Real earthquake 473/2123\n",
      "Real earthquake 474/2123\n",
      "Real earthquake 475/2123\n",
      "Real earthquake 476/2123\n",
      "Real earthquake 477/2123\n",
      "Real earthquake 478/2123\n",
      "Real earthquake 479/2123\n",
      "Real earthquake 480/2123\n",
      "Real earthquake 481/2123\n",
      "Real earthquake 482/2123\n",
      "Real earthquake 483/2123\n",
      "Real earthquake 484/2123\n",
      "Real earthquake 485/2123\n",
      "Real earthquake 486/2123\n",
      "Real earthquake 487/2123\n",
      "Real earthquake 488/2123\n",
      "Real earthquake 489/2123\n",
      "Real earthquake 490/2123\n",
      "Real earthquake 491/2123\n",
      "Real earthquake 492/2123\n",
      "Real earthquake 493/2123\n",
      "Real earthquake 494/2123\n",
      "Real earthquake 495/2123\n",
      "Real earthquake 496/2123\n",
      "Real earthquake 497/2123\n",
      "Real earthquake 498/2123\n",
      "Real earthquake 499/2123\n",
      "Real earthquake 500/2123\n",
      "Real earthquake 501/2123\n",
      "Real earthquake 502/2123\n",
      "Real earthquake 503/2123\n",
      "Real earthquake 504/2123\n",
      "Real earthquake 505/2123\n",
      "Real earthquake 506/2123\n",
      "Real earthquake 507/2123\n",
      "Real earthquake 508/2123\n",
      "Real earthquake 509/2123\n"
     ]
    }
   ],
   "source": [
    "nexamples = 5 # Number of examples to look at \n",
    "  \n",
    "# for ind in range(nexamples):\n",
    "# for ind in rows_w_eqs[:5]:\n",
    "counter = 0\n",
    "for ind in rows_w_eqs: # To save all earthquake examples\n",
    "#     print(ind)\n",
    "    counter += 1\n",
    "    print('Real earthquake ' + str(counter) + '/' + str(len(rows_w_eqs)))\n",
    "    \n",
    "    fig = plt.subplots(nrows = 1, ncols = 3, figsize = (18,4), dpi = 300)\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    t = 1/sr * np.arange(realtest_orig_data.shape[1])\n",
    "    # print(t)\n",
    "    \n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.plot(t, realtest_norm_data[ind,:,0]*100, label = 'N test data', color = 'C0')\n",
    "    ax1.set_ylabel('Displacement (cm)')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, realtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax2.plot(t, realtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.set_ylim(-0.05,1.05)\n",
    "    ax2.legend(loc = 'upper left')\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.set_title('Row with earthquake ' + str(ind), fontsize = 16)\n",
    "    ax3.plot(t, realtest_norm_data[ind,:,1]*100, label = 'E test data', color = 'C1')\n",
    "    ax3.set_ylabel('Displacement (cm)')\n",
    "    ax3.legend(loc = 'upper right')\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(t, realtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax4.plot(t, realtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax4.set_ylim(-0.05,1.05)\n",
    "    ax4.legend(loc = 'upper left')\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(t, realtest_norm_data[ind,:,2]*100, label = 'Z test data', color = 'C2')\n",
    "    ax5.set_ylabel('Displacement (cm)')\n",
    "    ax5.legend(loc = 'upper right')\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(t, realtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax6.plot(t, realtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax6.set_ylim(-0.05,1.05)\n",
    "    ax6.legend(loc = 'upper left')\n",
    "    \n",
    "#     plt.show()\n",
    "#     plt.savefig(figure_save_dir + '16_realtestdata_ex' + str(ind) + '_plot_predictions.png', format = 'PNG')\n",
    "    plt.savefig(figure_save_dir + 'realtestdata_alltrueeq_wfvplots/row_' + str(ind) + '_plot_predictions.png', format = 'PNG')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5fa93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c6c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87f862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54794c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0eacc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4b923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58643005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b033909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f83603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166a40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567176d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e83aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202a207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##### -------------------- CLASSIFICATION TESTS -------------------- #####\n",
    "\n",
    "# print('DOING CLASSIFICATION TESTS ON REAL DATA')\n",
    "\n",
    "# # Decision threshold evaluation\n",
    "\n",
    "# thresholds = np.arange(0, 1.005, 0.005)\n",
    "# # thresholds = np.arange(0, 1, 0.1)\n",
    "# test_thresholds = [0]\n",
    "\n",
    "# # Use np.where to see whether anywhere in test_predictions is > threshold\n",
    "# # If there is a value that's >, the 'result' of the array is 1. If not 0\n",
    "# # Then compare these 1s and 0s to the target array value for PAR\n",
    "\n",
    "# accuracies = []\n",
    "# accuracies_per = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# F1s = []\n",
    "\n",
    "# for threshold in thresholds:\n",
    "    \n",
    "#     # print('-------------------------------------------------------------')\n",
    "#     # print('Threshold: ' + str(threshold))\n",
    "#     # print('-------------------------------------------------------------')\n",
    "#     # print(' ')\n",
    "    \n",
    "#     # Convert the predictions arrays to single ones and zeroes\n",
    "    \n",
    "#     pred_binary = np.zeros(len(realtest_predictions))\n",
    "#     iterate = np.arange(0,len(realtest_predictions),1)\n",
    "    \n",
    "#     for k in iterate:\n",
    "#         # print('Prediction: ' + str(test_predictions[k]))\n",
    "#         i = np.where(realtest_predictions[k] >= threshold)[0]\n",
    "#         # print(i)\n",
    "#         if len(i) == 0:\n",
    "#             pred_binary[k] = 0\n",
    "#         elif len(i) > 0:\n",
    "#             pred_binary[k] = 1\n",
    "    \n",
    "#     # print('Predictions: ')\n",
    "#     # print(pred_binary)\n",
    "#     # print(pred_binary.shape)\n",
    "    \n",
    "#     # Convert the target arrays to single ones and zeroes\n",
    "    \n",
    "#     targ_binary = np.zeros(len(gauss_target)) # Need to make this ones at indices in rows_w_eqs\n",
    "    \n",
    "#     for idx in range(len(targ_binary)):\n",
    "        \n",
    "#         if idx in rows_w_eqs:\n",
    "            \n",
    "#             targ_binary[idx] = 1\n",
    "    \n",
    "#     # print('Targets: ')\n",
    "#     # print(targ_binary)\n",
    "    \n",
    "#     # Calculating the accuracy, precision, recall, and F1\n",
    "    \n",
    "#     num_preds = len(realtest_predictions) # total number of predictions. Amanda did 50\n",
    "#     correct_preds = []\n",
    "#     wrong_preds = []\n",
    "#     true_pos = []\n",
    "#     true_neg = []\n",
    "#     false_pos = []\n",
    "#     false_neg = []\n",
    "    \n",
    "#     for i in iterate:\n",
    "        \n",
    "#         pred = pred_binary[i]\n",
    "#         targ = targ_binary[i]\n",
    "        \n",
    "#         if pred == targ: # add one to list of correct predictions if matching\n",
    "#             correct_preds.append(1)\n",
    "            \n",
    "#             if pred == 1 and targ == 1:\n",
    "#                 true_pos.append(1)\n",
    "#             elif pred == 0 and targ == 0:\n",
    "#                 true_neg.append(1)\n",
    "            \n",
    "#         elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "#             wrong_preds.append(1)\n",
    "            \n",
    "#             if pred == 1 and targ == 0:\n",
    "#                 false_pos.append(1)\n",
    "#             elif pred == 0 and targ == 1:\n",
    "#                 false_neg.append(1)\n",
    "    \n",
    "#     num_correct_preds = len(correct_preds)\n",
    "#     num_wrong_preds = len(wrong_preds)\n",
    "#     num_true_pos = len(true_pos)\n",
    "#     num_true_neg = len(true_neg)\n",
    "#     num_false_pos = len(false_pos)\n",
    "#     num_false_neg = len(false_neg)\n",
    "    \n",
    "#     # print('Correct preds: ' + str(num_correct_preds))\n",
    "#     # print('Wrong preds: ' + str(num_wrong_preds))\n",
    "#     # print('True pos: ' + str(num_true_pos))\n",
    "#     # print('True neg: ' + str(num_true_neg))\n",
    "#     # print('False pos: ' + str(num_false_pos))\n",
    "#     # print('False neg: ' + str(num_false_neg))\n",
    "    \n",
    "#     # print('Threshold: ' + str(threshold))\n",
    "#     # print('Correct preds: ' + str(num_correct_preds))\n",
    "#     # print('Wrong preds: ' + str(num_wrong_preds))\n",
    "#     # print('True pos: ' + str(num_true_pos))\n",
    "#     # print('True neg: ' + str(num_true_neg))\n",
    "#     # print('False pos: ' + str(num_false_pos))\n",
    "#     # print('False neg: ' + str(num_false_neg))\n",
    "    \n",
    "#     accuracy = num_correct_preds / num_preds\n",
    "#     accuracy_per = (num_correct_preds / num_preds) * 100\n",
    "#     # print('Accuracy: ' + str(accuracy_per) + '%')\n",
    "    \n",
    "#     if num_true_pos == 0  and num_false_pos == 0:\n",
    "#         precision = 0\n",
    "#     else:\n",
    "#         precision = num_true_pos / (num_true_pos + num_false_pos)\n",
    "    \n",
    "#     if num_true_pos == 0 and num_false_neg == 0:\n",
    "#         recall = 0\n",
    "#     else:\n",
    "#         recall = num_true_pos / (num_true_pos + num_false_neg)\n",
    "    \n",
    "#     if precision + recall == 0:\n",
    "#         F1 = 0\n",
    "#     else:\n",
    "#         F1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "#     accuracies.append(accuracy)\n",
    "#     accuracies_per.append(accuracy_per)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     F1s.append(F1)\n",
    "\n",
    "# # print('Accuracies')\n",
    "# # print(accuracies)\n",
    "# # print('Precisions')\n",
    "# # print(precisions)\n",
    "# # print('Recalls')\n",
    "# # print(recalls)\n",
    "# # print('F1s')\n",
    "# # print(F1s)\n",
    "\n",
    "# np.savetxt('realdata_accuracies_percentage_txt.txt', accuracies_per)\n",
    "# np.savetxt('realdata_thresholds_txt.txt', thresholds)\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# # plt.scatter(thresholds,accuracies)\n",
    "# plt.plot(thresholds, accuracies_per, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('Accuracy (%)', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,100)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('Accuracy Percentage', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_accuracies_' + str(num_test) + '.png', format='PNG')\n",
    "# # plt.savefig('/Users/sydneydybing/Documents/AGU_2021/Figures/6_accuracies_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/accuracies_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/accuracies_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# plt.plot(thresholds, precisions, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('Precision', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('Precision', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_precisions_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/precisions_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/precisions_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# plt.plot(thresholds, recalls, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('Recall', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('Recall', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_recalls_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/recalls_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/recalls_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# plt.plot(thresholds, F1s, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('F1', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('F1', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_F1s_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/F1s_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/F1s_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# # ##### -------------------- GAUSSIAN PEAK POSITION TEST -------------------- #####\n",
    "\n",
    "# # print('DOING PEAK POSITION TESTS')\n",
    "\n",
    "# # # print(target[4])\n",
    "# # # print(test_predictions[4])\n",
    "\n",
    "# # thresholds = np.array([0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])\n",
    "\n",
    "# # # threshold = 0.2\n",
    "\n",
    "# # iterate = np.arange(0,len(realtest_predictions),1)\n",
    "# # s = 0\n",
    "\n",
    "# # fig = plt.subplots(nrows = 3, ncols = 4, figsize = (18,14), dpi = 400)\n",
    "# # # fig = plt.subplots(nrows = 3, ncols = 4, figsize = (18,14))\n",
    "# # plt.suptitle('Target vs. Prediction Samples Off per Threshold', fontsize = 20)\n",
    "\n",
    "# # for idx in range(len(thresholds)):\n",
    "    \n",
    "# #     threshold = thresholds[idx]\n",
    "\n",
    "# #     pred_binary = np.zeros(len(realtest_predictions))\n",
    "# #     iterate = np.arange(0,len(realtest_predictions),1)\n",
    "    \n",
    "# #     for k in iterate:\n",
    "# #         i = np.where(realtest_predictions[k] >= threshold)[0]\n",
    "# #         if len(i) == 0:\n",
    "# #             pred_binary[k] = 0\n",
    "# #         elif len(i) > 0:\n",
    "# #             pred_binary[k] = 1\n",
    "    \n",
    "# #     # print('Predictions: ')\n",
    "# #     # print(pred_binary)\n",
    "    \n",
    "# #     # Convert the target arrays to single ones and zeroes\n",
    "    \n",
    "# #     targ_binary = np.zeros(len(gauss_target)) # Need to make this ones at indices in rows_w_eqs\n",
    "    \n",
    "# #     for idx in range(len(targ_binary)):\n",
    "        \n",
    "# #         if idx in rows_w_eqs:\n",
    "            \n",
    "# #             targ_binary[idx] = 1\n",
    "    \n",
    "# #     # print('Targets: ')\n",
    "# #     # print(targ_binary)\n",
    "    \n",
    "# #     signals = []\n",
    "    \n",
    "# #     for i in iterate:\n",
    "# #         pred = pred_binary[i]\n",
    "# #         targ = targ_binary[i]\n",
    "        \n",
    "# #         # print(pred)\n",
    "# #         # print(targ)\n",
    "        \n",
    "# #         if pred == 1 and targ == 1: # True positive, there was a signal and it found it\n",
    "# #             signals.append(i) # Grab index from list of events that are correct and have a pick\n",
    "# #         else:\n",
    "# #             pass\n",
    "    \n",
    "# #     # print(signals)\n",
    "    \n",
    "# #     samples_off_list = []\n",
    "    \n",
    "# #     for index in signals:\n",
    "        \n",
    "# #         # Find the peak and then the index where that peak is and compare \n",
    "        \n",
    "# #         # print('----------------------')\n",
    "# #         # print('Signal number: ' + str(index))\n",
    "        \n",
    "# #         target_max_idx = np.argmax(gauss_target[index])\n",
    "# #         # print('Target: ' + str(target_max_idx))\n",
    "        \n",
    "# #         pred_max_idx = np.argmax(realtest_predictions[index])\n",
    "# #         # print('Prediction: ' + str(pred_max_idx))\n",
    "        \n",
    "# #         samples_off = np.abs(pred_max_idx - target_max_idx)\n",
    "# #         # print('Samples off: ' + str(samples_off))\n",
    "# #         samples_off_list.append(samples_off)\n",
    "        \n",
    "# #     # print(samples_off_list)\n",
    "    \n",
    "# #     plt.subplot(3,4,idx+1)\n",
    "# #     plt.hist(samples_off_list, bins=128, range=(0,128), label = 'Threshold: ' + str(threshold))\n",
    "# #     plt.xlim(0,128)\n",
    "# #     plt.ylim(0,300)\n",
    "# #     plt.legend()\n",
    "# #     plt.grid(which = 'major', color = 'lightgray')\n",
    "# #     plt.subplots_adjust(hspace = 0, wspace = 0)\n",
    "\n",
    "# #     if idx == 0:\n",
    "# #         plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "    \n",
    "# #     elif idx == 4:\n",
    "# #         plt.ylabel('Number of examples in bin')\n",
    "# #         plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "# #         plt.yticks([0, 50, 100, 150, 200, 250])\n",
    "        \n",
    "# #     elif idx == 8:\n",
    "# #         plt.yticks([0, 50, 100, 150, 200, 250])\n",
    "        \n",
    "# #     elif idx == 9:\n",
    "# #         plt.xlabel('Numbers of samples off target position')\n",
    "# #         plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "        \n",
    "# #     elif idx == 10:     \n",
    "# #         plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "        \n",
    "# #     else:\n",
    "# #         plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "# #         plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "    \n",
    "# #     plt.subplot(3,4,12)\n",
    "# #     plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "# #     plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "\n",
    "# # # plt.savefig(local_dir + 'plots/' + name + '/7_histogram.png', format='PNG')\n",
    "# # plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/10_realdata_peakpos_stats/histogram.png', format='PNG')\n",
    "# # # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/8_peakpos_stats/histogram.png', format='PNG')\n",
    "# # plt.close()\n",
    "\n",
    "# # ##### -------------------- METADATA ANALYSIS -------------------- #####\n",
    "\n",
    "# # print('DOING REAL METADATA ANALYSIS')\n",
    "\n",
    "# # print(stack_data.shape) # the data\n",
    "# # print(norm_real_meta_data.shape) # the metadata\n",
    "# # print(realtest_predictions.shape) # the model's predictions about the data\n",
    "\n",
    "# # # np.save('batch_out_11_10_21.npy', batch_out)  \n",
    "# # # np.save('test_preds_11_10_21.npy', test_predictions)  \n",
    "# # # np.save('target_11_10_21.npy', target)\n",
    "\n",
    "# # # print(batch_out[0])\n",
    "# # # print(metadata[0][0])\n",
    "# # # print(test_predictions[0])\n",
    "\n",
    "# # # print(metadata)\n",
    "\n",
    "# # zeros = np.zeros((realtest_predictions.shape[0],1))\n",
    "# # analysis_array = np.c_[norm_real_meta_data,zeros]\n",
    "\n",
    "# # # Metadata columns: station, date, start time, end time, counter, gauss position, pgd, SNR N component, SNR E, SNR Z\n",
    "\n",
    "# # # print(analysis_array.shape)\n",
    "\n",
    "# # for i in range(len(stack_data)):\n",
    "    \n",
    "# #     # print(i)\n",
    "    \n",
    "# #     # print(metadata[i])\n",
    "\n",
    "# #     if norm_real_meta_data[i][5] == 'nan':\n",
    "# #         # print(str(i) + ' is not an earthquake')\n",
    "# #         # analysis_array[i][3] = 'nan'\n",
    "        \n",
    "# #         threshold = 0.16\n",
    "        \n",
    "# #         # True positive, true negative, false positive, false negative\n",
    "        \n",
    "# #         # print('Threshold: ' + str(threshold))\n",
    "    \n",
    "# #         # Convert the predictions arrays to single ones and zeroes\n",
    "        \n",
    "# #         p = np.where(test_predictions[i] >= threshold)[0]\n",
    "# #         if len(p) == 0:\n",
    "# #             pred_binary = 0\n",
    "# #         elif len(p) > 0:\n",
    "# #             pred_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Prediction: ')\n",
    "# #         #     print(pred_binary)\n",
    "        \n",
    "# #         # # Convert the target arrays to single ones and zeroes\n",
    "        \n",
    "# #         t = np.where(target[i] > 0)[0]\n",
    "# #         if len(t) == 0:\n",
    "# #             targ_binary = 0\n",
    "# #         elif len(t) > 0:\n",
    "# #             targ_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Target: ')\n",
    "# #         #     print(targ_binary)\n",
    "        \n",
    "# #         pred = pred_binary\n",
    "# #         targ = targ_binary\n",
    "        \n",
    "# #         if pred == targ: # add one to list of correct predictions if matching\n",
    "# #             # correct_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 1:\n",
    "# #                 result = 'true pos'\n",
    "# #             elif pred == 0 and targ == 0:\n",
    "# #                 result = 'true neg'\n",
    "            \n",
    "# #         elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "# #             # wrong_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 0:\n",
    "# #                 result = 'false pos'\n",
    "# #             elif pred == 0 and targ == 1:\n",
    "# #                 result = 'false neg'\n",
    "        \n",
    "# #         analysis_array[i][3] = result\n",
    "    \n",
    "# #     else:\n",
    "# #         # print(str(i) + ' is an earthquake')\n",
    "        \n",
    "# #         rupt_num = metadata[i][0]\n",
    "# #         station = metadata[i][1]\n",
    "# #         mag = metadata[i][2]\n",
    "        \n",
    "# #         # print(rupt_num)\n",
    "# #         # print(station)\n",
    "# #         # print(mag)\n",
    "        \n",
    "# #         # print(batch_out[i])\n",
    "# #         # print(test_predictions[i])\n",
    "# #         # plt.plot(test_predictions[i])\n",
    "# #         # plt.show()\n",
    "        \n",
    "# #         threshold = 0.2\n",
    "        \n",
    "# #         # True positive, true negative, false positive, false negative\n",
    "        \n",
    "# #         # print('Threshold: ' + str(threshold))\n",
    "    \n",
    "# #         # Convert the predictions arrays to single ones and zeroes\n",
    "        \n",
    "# #         p = np.where(test_predictions[i] >= threshold)[0]\n",
    "# #         if len(p) == 0:\n",
    "# #             pred_binary = 0\n",
    "# #         elif len(p) > 0:\n",
    "# #             pred_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Prediction: ')\n",
    "# #         #     print(pred_binary)\n",
    "        \n",
    "# #         # # Convert the target arrays to single ones and zeroes\n",
    "        \n",
    "# #         t = np.where(target[i] > 0)[0]\n",
    "# #         if len(t) == 0:\n",
    "# #             targ_binary = 0\n",
    "# #         elif len(t) > 0:\n",
    "# #             targ_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Target: ')\n",
    "# #         #     print(targ_binary)\n",
    "        \n",
    "# #         pred = pred_binary\n",
    "# #         targ = targ_binary\n",
    "        \n",
    "# #         if pred == targ: # add one to list of correct predictions if matching\n",
    "# #             # correct_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 1:\n",
    "# #                 result = 'true pos'\n",
    "# #             elif pred == 0 and targ == 0:\n",
    "# #                 result = 'true neg'\n",
    "            \n",
    "# #         elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "# #             # wrong_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 0:\n",
    "# #                 result = 'false pos'\n",
    "# #             elif pred == 0 and targ == 1:\n",
    "# #                 result = 'false neg'\n",
    "        \n",
    "# #         analysis_array[i][3] = result\n",
    "    \n",
    "# # print(analysis_array)\n",
    "# # print(analysis_array.shape)\n",
    "            \n",
    "# # # # np.save('/home/sdybing/GNSS_project/' + name + 'testing_for_analysis.npy', analysis_array) # VAL\n",
    "# # np.save('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/testing_for_analysis.npy', analysis_array) # LAP        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
