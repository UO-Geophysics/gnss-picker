{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47688b33",
   "metadata": {},
   "source": [
    "## Imports and Path/Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153a368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 14:16:31.964068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-24 14:16:31.964093: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from scipy import signal\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import os\n",
    "import X_gnss_unet_datagen_fn9 # Module with CNN/data generator code\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "project_name = 'july6' # Based on the name of the FakeQuakes project\n",
    "fq_dir = '/hdd/rc_fq/summer23/' # Where are the FakeQuakes stored? (The final .hdf5 file)\n",
    "noise_dir = '/home/sdybing/gnss-picker/data/noisedata/' # Where is the noise data stored?\n",
    "realdata_dir = '/home/sdybing/gnss-picker/data/realdata/' # Where is the noise data stored?\n",
    "\n",
    "cnn_save_dir = '/home/sdybing/gnss-picker/cnn_models_outputs/' # Where do you want to save this code's outputs?\n",
    "project_save_dir = cnn_save_dir + project_name + '_fq_train/'\n",
    "base_figure_save_dir = project_save_dir + 'base_data_figures/' # Where to save the figures of just the data/generator tests\n",
    "models_path = project_save_dir + 'models/'\n",
    "if os.path.isdir(project_save_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(project_save_dir)\n",
    "    os.makedirs(base_figure_save_dir)\n",
    "    os.makedirs(models_path)\n",
    "    \n",
    "train = False # Do you want to train?\n",
    "drop = 1 # Drop?\n",
    "resume = 0 # Resume training\n",
    "large = 0.5 # Large unet\n",
    "fac = large\n",
    "epochs = 100 # How many epochs?\n",
    "std = 3 # How long do you want the Gaussian STD to be?\n",
    "sr = 1 # Sample rate (Hz)\n",
    "epsilon = 1e-6\n",
    "batch_size = 32\n",
    "load = True # Loading an old trained model?\n",
    "small_train = False # Train with a smaller amount of data to make sure code works?\n",
    "small_test = False # Test with a smaller amount of data to make sure code works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab514d",
   "metadata": {},
   "source": [
    "## Data Loading and Formatting\n",
    "\n",
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddcd4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FakeQuakes...\n",
      "Loading FakeQuakes metadata...\n",
      "Loading noise...\n",
      "Loading real data...\n",
      "Loading real metadata...\n",
      "FakeQuakes shape: (917400, 768)\n",
      "Noise data shape: (917400, 768)\n",
      "Real data shape: (1061760, 384)\n"
     ]
    }
   ],
   "source": [
    "# FakeQuakes waveform data\n",
    "print('Loading FakeQuakes...')\n",
    "fq_data = h5py.File(fq_dir + 'july6_128samps_fq_wvfm_data_formatted.hdf5', 'r')\n",
    "fq_data = fq_data['data'][:,:]\n",
    "# print(x_data.shape)\n",
    "\n",
    "# FakeQuakes metadata\n",
    "print('Loading FakeQuakes metadata...')\n",
    "fq_metadata = np.load(fq_dir + 'july6_128samps_fq_wvfm_info.npy')\n",
    "\n",
    "# Noise data\n",
    "print('Loading noise...')\n",
    "all_noise_data = h5py.File(noise_dir + 'summer23_128samps_all_noise_samples.hdf5', 'r')\n",
    "all_noise_data = all_noise_data['all_noise_samples'][:,:]\n",
    "\n",
    "# Normalized real waveform data\n",
    "print('Loading real data...')\n",
    "real_data = h5py.File(realdata_dir + 'norm_realdata.hdf5', 'r')\n",
    "real_data = real_data['norm_realdata'][:,:]\n",
    "\n",
    "# Real metadata\n",
    "print('Loading real metadata...')\n",
    "real_metadata = np.load(realdata_dir + 'realdata_info.npy', allow_pickle = True)\n",
    "\n",
    "# Trim noise data to match length of FakeQuakes data\n",
    "noise_data = all_noise_data[:len(fq_data)]\n",
    "\n",
    "# Array of NaNs to use to match added noise in concatenation later\n",
    "nan_array = np.empty((len(fq_data), 3))\n",
    "nan_array[:] = np.NaN\n",
    "\n",
    "# Real data\n",
    "\n",
    "# Check shapes\n",
    "print('FakeQuakes shape: ' + str(fq_data.shape))\n",
    "print('Noise data shape: ' + str(noise_data.shape))\n",
    "print('Real data shape: ' + str(real_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bbd649b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ACSB', '20190704', '2019-07-04T00:00:00.000000Z',\n",
       "        '2019-07-04T00:02:07.000000Z', '0'],\n",
       "       ['ACSB', '20190704', '2019-07-04T00:02:07.000000Z',\n",
       "        '2019-07-04T00:04:14.000000Z', '1'],\n",
       "       ['ACSB', '20190704', '2019-07-04T00:04:14.000000Z',\n",
       "        '2019-07-04T00:06:21.000000Z', '2'],\n",
       "       ...,\n",
       "       ['WKPK', '20200604', '2020-06-04T23:52:59.000000Z',\n",
       "        '2020-06-04T23:55:06.000000Z', '677'],\n",
       "       ['WKPK', '20200604', '2020-06-04T23:55:06.000000Z',\n",
       "        '2020-06-04T23:57:13.000000Z', '678'],\n",
       "       ['WKPK', '20200604', '2020-06-04T23:57:13.000000Z',\n",
       "        '2020-06-04T23:59:20.000000Z', '679']], dtype='<U27')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b0d122",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4897297 into shape (1061760,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m real_metadata2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrealdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal_metadata_w_gauss_pos.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:430\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/format.py:786\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 786\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 4897297 into shape (1061760,6)"
     ]
    }
   ],
   "source": [
    "real_metadata2 = np.load(realdata_dir + 'real_metadata_w_gauss_pos.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_metadata2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a5a0f",
   "metadata": {},
   "source": [
    "### Format and Split Training, Validation, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2910c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full FakeQuakes data shape: (917400,)\n",
      "FakeQuakes training data shape: (733920,)\n",
      "FakeQuakes validation data shape: (91740,)\n",
      "FakeQuakes testing data shape: (91740,)\n",
      "Full noise data shape: (917400,)\n",
      "Noise training data shape: (733920,)\n",
      "Noise validation data shape: (91740,)\n",
      "Noise testing data shape: (91740,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(27)\n",
    "\n",
    "# Earthquake/signal data\n",
    "fqinds = np.arange(fq_data.shape[0]) # Signal indices\n",
    "np.random.shuffle(fqinds) # Shuffles the indices\n",
    "train_fqinds = fqinds[:int(0.8*len(fqinds))] # Training data separation: grabs the front 80% of the numbers\n",
    "valid_fqinds = fqinds[int(0.8*len(fqinds)):int(0.9*len(fqinds)):] # Grabs the next 10% (80-90%)\n",
    "test_fqinds = fqinds[int(0.9*len(fqinds)):] # Grabs the last 10% (90%-end)\n",
    "\n",
    "# Noise data\n",
    "noiseinds = np.arange(noise_data.shape[0]) # Noise indices\n",
    "np.random.shuffle(noiseinds) # Shuffles the indices\n",
    "train_noiseinds = noiseinds[:int(0.8*len(noiseinds))] # Data separation as above\n",
    "valid_noiseinds = noiseinds[int(0.8*len(noiseinds)):int(0.9*len(noiseinds))]\n",
    "test_noiseinds = noiseinds[int(0.9*len(noiseinds)):]\n",
    "\n",
    "# Check shapes to confirm compatability\n",
    "print('Full FakeQuakes data shape: ' + str(fqinds.shape))\n",
    "print('FakeQuakes training data shape: ' + str(train_fqinds.shape))\n",
    "print('FakeQuakes validation data shape: ' + str(valid_fqinds.shape))\n",
    "print('FakeQuakes testing data shape: ' + str(test_fqinds.shape))\n",
    "print('Full noise data shape: ' + str(noiseinds.shape))\n",
    "print('Noise training data shape: ' + str(train_noiseinds.shape))\n",
    "print('Noise validation data shape: ' + str(valid_noiseinds.shape))\n",
    "print('Noise testing data shape: ' + str(test_noiseinds.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce024b",
   "metadata": {},
   "source": [
    "### Check Loaded Data with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba1d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the FakeQuakes data\n",
    "plt.figure(figsize = (8,5))   \n",
    "plt.title('Earthquake test', fontsize = 14)\n",
    "for idx in range(10): # plot 10 of them\n",
    "    plt.plot(fq_data[idx,:] / np.max(np.abs(fq_data[idx,:])) + idx) # Normalized and offset for each idx\n",
    "plt.axvline(256.5, linestyle = '--', color = 'lightgray')\n",
    "plt.axvline(513.5, linestyle = '--', color = 'lightgray')\n",
    "plt.xlabel('Time (s)', fontsize = 12)\n",
    "plt.ylabel('Normalized amplitude', fontsize = 12)\n",
    "plt.xlim(0,770)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(x = 5, y = -0.75, s = 'N', fontsize = 20)\n",
    "plt.text(x = 261, y = -0.75, s = 'E', fontsize = 20)\n",
    "plt.text(x = 518, y = -0.75, s = 'Z', fontsize = 20)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(base_figure_save_dir + '1_plot_raw_eq_data.png', format = 'PNG')\n",
    "plt.close()\n",
    "\n",
    "# Plot noise to check\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.title('Noise test', fontsize = 14)\n",
    "for idx in range(10):\n",
    "    plt.plot(noise_data[idx,:] / np.max(np.abs(noise_data[idx,:])) + idx)\n",
    "plt.axvline(256.5, linestyle = '--', color = 'lightgray')\n",
    "plt.axvline(513.5, linestyle = '--', color = 'lightgray')\n",
    "plt.xlabel('Time (s)', fontsize = 12)\n",
    "plt.ylabel('Normalized amplitude', fontsize = 12)\n",
    "plt.xlim(0,770)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(x = 5, y = -1.25, s = 'N', fontsize = 20)\n",
    "plt.text(x = 261, y = -1.25, s = 'E', fontsize = 20)\n",
    "plt.text(x = 518, y = -1.25, s = 'Z', fontsize = 20)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(base_figure_save_dir + '2_plot_noise_data.png', format = 'PNG')\n",
    "plt.close()\n",
    "\n",
    "# Check the PGD distribution\n",
    "\n",
    "# testing_data = fq_data[test_fqinds]\n",
    "\n",
    "# pgd = np.zeros(testing_data.shape[0]) # Reminder - FQ data is in meters\n",
    "# for idx in range(testing_data.shape[0]):\n",
    "#     pgd[idx] = np.max(np.sqrt((testing_data[idx,:257])**2 + (testing_data[idx,257:514])**2 + (testing_data[idx,514:])**2))\n",
    "\n",
    "# plt.figure(figsize = (8,5))\n",
    "# plt.hist(pgd, bins = 30, alpha = 0.5, edgecolor = 'black')\n",
    "# plt.ylim(0,10000)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c113ec",
   "metadata": {},
   "source": [
    "## Test of Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b594ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generator check data shape: (32, 128, 3)\n",
      "Data generator check target shape: (32, 128)\n",
      "Data generator check metadata shape: (32, 3)\n"
     ]
    }
   ],
   "source": [
    "checkgen = X_gnss_unet_datagen_fn9.my_3comp_data_generator(32, fq_data, noise_data, fq_metadata, nan_array, train_fqinds, train_noiseinds, sr, std, valid = True) # Valid = True to get original data back\n",
    "checkgen_data, checkgen_target, checkgen_metadata = next(checkgen) \n",
    "\n",
    "print('Data generator check data shape: ' + str(checkgen_data.shape))\n",
    "print('Data generator check target shape: ' + str(checkgen_target.shape))\n",
    "print('Data generator check metadata shape: ' + str(checkgen_metadata.shape))\n",
    "\n",
    "# Shapes:\n",
    "    # data: (batch_size, 128, 3) # N, E, Z\n",
    "    # target: (batch_size, 128)\n",
    "    # metadata: (batch_size, 3) Rupt name, station name, magnitude\n",
    "\n",
    "# Plot generator results\n",
    "\n",
    "nexamples = 5 # Number of examples to look at \n",
    "  \n",
    "for ind in range(nexamples): \n",
    "    \n",
    "#     print('Magnitude: ' + str(metadata[ind,2]))\n",
    "\n",
    "    fig = plt.subplots(nrows = 1, ncols = 3, figsize = (26,4), dpi = 300) # shoter for AGU talk\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    t = 1/sr * np.arange(checkgen_data.shape[1])\n",
    "    \n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.plot(t, checkgen_data[ind,:,0]*100, label = 'N original data', color = 'C0')\n",
    "    ax1.set_ylabel('Displacement (cm)')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, checkgen_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.legend(loc = 'lower right')\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.plot(t, checkgen_data[ind,:,1]*100, label = 'E original data', color = 'C1')\n",
    "    ax3.set_ylabel('Displacement (cm)')\n",
    "    ax3.legend(loc = 'upper right')\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(t, checkgen_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax4.legend(loc = 'lower right')\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(t, checkgen_data[ind,:,2]*100, label = 'Z original data', color = 'C2')\n",
    "    ax5.set_ylabel('Displacement (cm)')\n",
    "    ax5.legend(loc = 'upper right')\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(t, checkgen_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax6.legend(loc = 'lower right')\n",
    "    \n",
    "#     plt.show()\n",
    "    plt.savefig(base_figure_save_dir + '3_ex' + str(ind) + '_plot_generator_pass.png', format = 'PNG')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b12f7",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b91607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 128, 16)      1024        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 64, 16)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 64, 32)       7712        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 32, 32)      0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 32, 64)       22592       ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 16, 64)      0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1024)         0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           16400       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 16, 1)        0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 16, 64)       768         ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 32, 64)       0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 128)      0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32, 32)       61472       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 64, 32)      0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64)       0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 64, 16)       21520       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling1d_2 (UpSampling1D)  (None, 128, 16)     0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 32)      0           ['up_sampling1d_2[0][0]',        \n",
      "                                                                  'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 128, 1)       673         ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 128)          0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,161\n",
      "Trainable params: 132,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Using model with dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 23:42:47.367189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.367427: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.367526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.367724: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.367883: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.368037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.368192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.368333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-14 23:42:47.368340: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-14 23:42:47.368775: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sdybing/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "if drop: # Use a model with a dropout layer\n",
    "    model = X_gnss_unet_datagen_fn9.make_large_unet_drop(fac, sr, ncomps = 3)\n",
    "    print('Using model with dropout')\n",
    "else:\n",
    "    model = X_gnss_unet_datagen_fn9.make_large_unet(fac, sr, ncomps = 3)  \n",
    "    print('Using large model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aff619",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a6c69",
   "metadata": {},
   "source": [
    "### See how training works with a smaller dataset (faster)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffea016",
   "metadata": {},
   "outputs": [],
   "source": [
    "if small_train:\n",
    "    train_fqinds = train_fqinds[:10000]\n",
    "    train_noiseinds = train_noiseinds[:10000]\n",
    "    valid_fqinds = valid_fqinds[:10000]\n",
    "    valid_noiseinds = valid_noiseinds[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915f6a0",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33537b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    \n",
    "    traindate = date.today()\n",
    "    \n",
    "    model_save_dir = models_path + 'traindate_' + str(traindate) + '/' # Where to save the trained model\n",
    "    data_save_dir = model_save_dir + 'data/' # Where to save the outputted testing data and predictions\n",
    "    figure_save_dir = model_save_dir + 'figures/' # Where to save the figures\n",
    "    \n",
    "    if os.path.isdir(model_save_dir):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(model_save_dir)\n",
    "        os.makedirs(data_save_dir)\n",
    "        os.makedirs(figure_save_dir)\n",
    "    \n",
    "    model_save_file = model_save_dir + 'bestmodel_traindate_' + str(traindate) + '.h5'\n",
    "    \n",
    "    print('Training model and saving results to ' + model_save_file)\n",
    "    \n",
    "    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown = 0, patience = 4, min_lr = 0.5e-6)\n",
    "    early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = model_save_file, monitor = 'val_loss', mode = 'auto', verbose = 1, save_best_only = True)\n",
    "    callbacks = [lr_reducer, early_stopping_monitor, checkpoint]\n",
    "    \n",
    "    history = model.fit(X_gnss_unet_datagen_fn9.my_3comp_data_generator(batch_size, fq_data, noise_data, fq_metadata, nan_array, train_fqinds, train_noiseinds, sr, std), # Valid = False for training; implied\n",
    "                        steps_per_epoch = (len(train_fqinds) + len(train_noiseinds))//batch_size,\n",
    "                        validation_data = X_gnss_unet_datagen_fn9.my_3comp_data_generator(batch_size, fq_data, noise_data, fq_metadata, nan_array, valid_fqinds, valid_noiseinds, sr, std),\n",
    "                        validation_steps = (len(valid_fqinds) + len(valid_noiseinds))//batch_size,\n",
    "                        epochs = epochs, callbacks = callbacks)\n",
    "    \n",
    "    model.save_weights(model_save_file)\n",
    "    np.save(model_save_dir + 'training_history.npy', history.history)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1ea2d",
   "metadata": {},
   "source": [
    "### Check Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd64ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    \n",
    "    history = np.load(model_save_dir + 'training_history.npy', allow_pickle = 'TRUE').item()\n",
    "\n",
    "    fig = plt.subplots(nrows = 2, ncols = 1, figsize = (6,8))\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.plot(history['loss'], label = 'Training loss')\n",
    "    ax1.plot(history['val_loss'], label = 'Validation loss') \n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Model: traindate_' + str(traindate) + '.h5')\n",
    "\n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.plot(history['accuracy'], label = 'Training accuracy') \n",
    "    ax2.plot(history['val_accuracy'], label = 'Validation accuracy') \n",
    "    ax2.legend(loc = 'lower right')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    plt.subplots_adjust(hspace = 0)\n",
    "\n",
    "    # plt.show()\n",
    "    plt.savefig(figure_save_dir + '4_training_curves.png', format = 'PNG')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d756c8d",
   "metadata": {},
   "source": [
    "## Load an old trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2358f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training results from /home/sdybing/gnss-picker/cnn_models_outputs/july6_fq_train/models/traindate_2023-07-27/bestmodel_traindate_2023-07-27.h5\n"
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    loaddate = '2023-07-27' # Format: YYYY-MM-DD\n",
    "    model_load_file = project_save_dir + 'models/traindate_' + str(loaddate) + '/bestmodel_traindate_' + str(loaddate) + '.h5'\n",
    "    data_save_dir = models_path + 'traindate_' + str(loaddate) + '/data/' # Where to save the outputted testing data and predictions\n",
    "    figure_save_dir = models_path + 'traindate_' + str(loaddate) + '/figures/'\n",
    "    print('Loading training results from ' + model_load_file)\n",
    "    \n",
    "    model.load_weights(model_load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6eadb",
   "metadata": {},
   "source": [
    "## Test the Model with Remaining FakeQuakes Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642828f8",
   "metadata": {},
   "source": [
    "### See how testing works with a smaller dataset (faster)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ca4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if small_test:\n",
    "    test_fqinds = test_fqinds[:100]\n",
    "    test_noiseinds = test_noiseinds[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a838dd",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731f4e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "FQ test data shape: (91738, 128, 3)\n",
      "FQ test metadata shape: (91738, 3)\n",
      "FQ test target shape: (91738, 128)\n",
      "FQ test predictions shape: (91738, 128)\n"
     ]
    }
   ],
   "source": [
    "fqtestdate = date.today()\n",
    "num_fqtest = len(test_fqinds) - 2 # Number of samples to test with\n",
    "# print(num_fqtest)\n",
    "\n",
    "fqtestmodel = X_gnss_unet_datagen_fn9.my_3comp_data_generator(num_fqtest, fq_data, noise_data, fq_metadata, nan_array, test_fqinds, test_noiseinds, sr, std, valid = True)\n",
    "fqtest_data, fqtest_target, fqtest_metadata = next(fqtestmodel)\n",
    "print('Predicting...')\n",
    "fqtest_predictions = model.predict(fqtest_data)\n",
    "\n",
    "print('FQ test data shape: ' + str(fqtest_data.shape))\n",
    "print('FQ test metadata shape: ' + str(fqtest_metadata.shape))\n",
    "print('FQ test target shape: ' + str(fqtest_target.shape))\n",
    "print('FQ test predictions shape: ' + str(fqtest_predictions.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f45e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "FQ test data shape: (91738, 128, 3)\n",
      "FQ test metadata shape: (91738, 3)\n",
      "FQ test target shape: (91738, 128)\n",
      "FQ test predictions shape: (91738, 128)\n"
     ]
    }
   ],
   "source": [
    "# Testing with validation data\n",
    "\n",
    "fqtestdate = date.today()\n",
    "num_fqtest = len(valid_fqinds) - 2 # Number of samples to test with\n",
    "# print(num_fqtest)\n",
    "\n",
    "fqtestmodel = X_gnss_unet_datagen_fn9.my_3comp_data_generator(num_fqtest, fq_data, noise_data, fq_metadata, nan_array, valid_fqinds, valid_noiseinds, sr, std, valid = True)\n",
    "fqtest_data, fqtest_target, fqtest_metadata = next(fqtestmodel)\n",
    "print('Predicting...')\n",
    "fqtest_predictions = model.predict(fqtest_data)\n",
    "\n",
    "print('FQ test data shape: ' + str(fqtest_data.shape))\n",
    "print('FQ test metadata shape: ' + str(fqtest_metadata.shape))\n",
    "print('FQ test target shape: ' + str(fqtest_target.shape))\n",
    "print('FQ test predictions shape: ' + str(fqtest_predictions.shape))\n",
    "\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqvalid_data.npy', fqtest_data)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqvalid_metadata.npy', fqtest_metadata)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqvalid_target.npy', fqtest_target)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqvalid_predictions.npy', fqtest_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc4ad6",
   "metadata": {},
   "source": [
    "### Save the FQ testing data, targets, metadata, and predictions as .npys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46376868",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_data.npy', fqtest_data)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_metadata.npy', fqtest_metadata)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_target.npy', fqtest_target)\n",
    "np.save(data_save_dir + str(fqtestdate) + '_fqtest_predictions.npy', fqtest_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceba3e5",
   "metadata": {},
   "source": [
    "### Check PGD distribution of FQ testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f96efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fqtest_pgd = np.zeros(fqtest_data.shape[0])\n",
    "for idx in range(fqtest_data.shape[0]):\n",
    "    fqtest_pgd[idx] = np.max(np.sqrt((fqtest_data[idx,:,0])**2 + (fqtest_data[idx,:,1])**2 + (fqtest_data[idx,:,2])**2))\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.hist(fqtest_pgd, bins = 30, alpha = 0.5, edgecolor = 'black')\n",
    "# plt.ylim(0,4000)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(figure_save_dir + '5a_fqtest_data_pgd_distrib.png', format = 'PNG')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff594f2",
   "metadata": {},
   "source": [
    "### Plot checks of FQ testing data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e81e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nexamples = 5 # Number of examples to look at \n",
    "  \n",
    "for ind in range(nexamples): \n",
    "    \n",
    "    fig = plt.subplots(nrows = 1, ncols = 3, figsize = (18,4), dpi = 300)\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    t = 1/sr * np.arange(fqtest_data.shape[1])\n",
    "    # print(t)\n",
    "    \n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.plot(t, fqtest_data[ind,:,0]*100, label = 'N test data', color = 'C0')\n",
    "    ax1.set_ylabel('Displacement (cm)')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.legend(loc = 'upper right')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(t, fqtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax2.plot(t, fqtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.set_ylim(-0.05,1.05)\n",
    "    ax2.legend(loc = 'upper left')\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.plot(t, fqtest_data[ind,:,1]*100, label = 'E test data', color = 'C1')\n",
    "    ax3.set_ylabel('Displacement (cm)')\n",
    "    ax3.legend(loc = 'upper right')\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(t, fqtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax4.plot(t, fqtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax4.set_ylim(-0.05,1.05)\n",
    "    ax4.legend(loc = 'upper left')\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(t, fqtest_data[ind,:,2]*100, label = 'Z test data', color = 'C2')\n",
    "    ax5.set_ylabel('Displacement (cm)')\n",
    "    ax5.legend(loc = 'upper right')\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(t, fqtest_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "    ax6.plot(t, fqtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "    ax6.set_ylim(-0.05,1.05)\n",
    "    ax6.legend(loc = 'upper left')\n",
    "    \n",
    "#     plt.show()\n",
    "    plt.savefig(figure_save_dir + '6_ex' + str(ind) + '_plot_test_predictions.png', format = 'PNG')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337289e",
   "metadata": {},
   "source": [
    "## Test the Model with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778674e4",
   "metadata": {},
   "source": [
    "### Run Real Data through Data Generator and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2de18ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # realtest = gnss_unet_tools.real_data_generator(data = real_data, data_inds = real_data_inds, meta_data = real_meta_data, sr = 1, std = 3, nlen = 128)\n",
    "# realtest = gnss_unet_tools.real_data_generator(data = norm_real_data, data_inds = norm_real_data_inds, meta_data = norm_real_meta_data, sr = 1, std = 3, nlen = 128)\n",
    "# stack_data, gauss_target = next(realtest)\n",
    "# # print(stack_data)\n",
    "# # print(stack_data.shape)\n",
    "# # print(gauss_target)\n",
    "# # print(gauss_target.shape)\n",
    "# realtest_predictions = model.predict(stack_data)\n",
    "\n",
    "# # np.save('realtest_predictions.npy', realtest_predictions)\n",
    "# # np.save('stack_data.npy', stack_data)\n",
    "# # np.save('gauss_target.npy', gauss_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428d47d",
   "metadata": {},
   "source": [
    "### Plot checks of FQ testing data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd9f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rows_w_eqs = np.load('real_metadata_rowsweqs.npy')\n",
    "# rows_w_eqs = np.load('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/More_RealData/rowsweqs.npy') # More data, normed\n",
    "\n",
    "# nexamples = 5 # Number of examples to look at \n",
    "  \n",
    "# # for ind in range(nexamples): \n",
    "# for ind in rows_w_eqs[43]:\n",
    "    \n",
    "#     fig = plt.subplots(nrows = 1, ncols = 3, figsize = (22,4), dpi = 300)\n",
    "#     plt.subplots_adjust(wspace = 0.4)\n",
    "#     t = 1/sr * np.arange(batch_out.shape[1])\n",
    "    \n",
    "#     ax1 = plt.subplot(131)\n",
    "#     ax1.plot(t, stack_data[ind,:,0]*100, label = 'N original data', color = 'C0')\n",
    "#     ax1.set_ylabel('Displacement (cm)')\n",
    "#     ax1.set_xlabel('Time (s)')\n",
    "#     ax1.legend(loc = 'upper right')\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(t, gauss_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "#     ax2.plot(t, realtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "#     ax2.set_ylabel('Confidence')\n",
    "#     ax2.set_ylim(-0.05,1.05)\n",
    "#     ax2.legend(loc = 'upper left')\n",
    "    \n",
    "#     ax3 = plt.subplot(132)\n",
    "#     ax3.plot(t, stack_data[ind,:,1]*100, label = 'E original data', color = 'C1')\n",
    "#     ax3.set_ylabel('Displacement (cm)')\n",
    "#     ax3.legend(loc = 'upper right')\n",
    "#     ax4 = ax3.twinx()\n",
    "#     ax4.plot(t, gauss_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "#     ax4.plot(t, realtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "#     ax4.set_ylim(-0.05,1.05)\n",
    "#     ax4.legend(loc = 'upper left')\n",
    "    \n",
    "#     ax5 = plt.subplot(133)\n",
    "#     ax5.plot(t, stack_data[ind,:,2]*100, label = 'Z original data', color = 'C2')\n",
    "#     ax5.set_ylabel('Displacement (cm)')\n",
    "#     ax5.legend(loc = 'upper right')\n",
    "#     ax6 = ax5.twinx()\n",
    "#     ax6.plot(t, gauss_target[ind,:], color = 'black', linestyle = '--', label = 'Target')\n",
    "#     ax6.plot(t, realtest_predictions[ind,:], color = 'red', linestyle = '--', label = 'Prediction')\n",
    "#     ax6.set_ylim(-0.05,1.05)\n",
    "#     ax6.legend(loc = 'upper left')\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "# #     plt.savefig(local_dir + 'plots/' + name + '/5_ex_' + str(ind) + '_plot_predictions.png', format = 'PNG')\n",
    "# #     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfe9ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##### -------------------- CLASSIFICATION TESTS -------------------- #####\n",
    "\n",
    "# print('DOING CLASSIFICATION TESTS ON REAL DATA')\n",
    "\n",
    "# # Decision threshold evaluation\n",
    "\n",
    "# thresholds = np.arange(0, 1.005, 0.005)\n",
    "# # thresholds = np.arange(0, 1, 0.1)\n",
    "# test_thresholds = [0]\n",
    "\n",
    "# # Use np.where to see whether anywhere in test_predictions is > threshold\n",
    "# # If there is a value that's >, the 'result' of the array is 1. If not 0\n",
    "# # Then compare these 1s and 0s to the target array value for PAR\n",
    "\n",
    "# accuracies = []\n",
    "# accuracies_per = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# F1s = []\n",
    "\n",
    "# for threshold in thresholds:\n",
    "    \n",
    "#     # print('-------------------------------------------------------------')\n",
    "#     # print('Threshold: ' + str(threshold))\n",
    "#     # print('-------------------------------------------------------------')\n",
    "#     # print(' ')\n",
    "    \n",
    "#     # Convert the predictions arrays to single ones and zeroes\n",
    "    \n",
    "#     pred_binary = np.zeros(len(realtest_predictions))\n",
    "#     iterate = np.arange(0,len(realtest_predictions),1)\n",
    "    \n",
    "#     for k in iterate:\n",
    "#         # print('Prediction: ' + str(test_predictions[k]))\n",
    "#         i = np.where(realtest_predictions[k] >= threshold)[0]\n",
    "#         # print(i)\n",
    "#         if len(i) == 0:\n",
    "#             pred_binary[k] = 0\n",
    "#         elif len(i) > 0:\n",
    "#             pred_binary[k] = 1\n",
    "    \n",
    "#     # print('Predictions: ')\n",
    "#     # print(pred_binary)\n",
    "#     # print(pred_binary.shape)\n",
    "    \n",
    "#     # Convert the target arrays to single ones and zeroes\n",
    "    \n",
    "#     targ_binary = np.zeros(len(gauss_target)) # Need to make this ones at indices in rows_w_eqs\n",
    "    \n",
    "#     for idx in range(len(targ_binary)):\n",
    "        \n",
    "#         if idx in rows_w_eqs:\n",
    "            \n",
    "#             targ_binary[idx] = 1\n",
    "    \n",
    "#     # print('Targets: ')\n",
    "#     # print(targ_binary)\n",
    "    \n",
    "#     # Calculating the accuracy, precision, recall, and F1\n",
    "    \n",
    "#     num_preds = len(realtest_predictions) # total number of predictions. Amanda did 50\n",
    "#     correct_preds = []\n",
    "#     wrong_preds = []\n",
    "#     true_pos = []\n",
    "#     true_neg = []\n",
    "#     false_pos = []\n",
    "#     false_neg = []\n",
    "    \n",
    "#     for i in iterate:\n",
    "        \n",
    "#         pred = pred_binary[i]\n",
    "#         targ = targ_binary[i]\n",
    "        \n",
    "#         if pred == targ: # add one to list of correct predictions if matching\n",
    "#             correct_preds.append(1)\n",
    "            \n",
    "#             if pred == 1 and targ == 1:\n",
    "#                 true_pos.append(1)\n",
    "#             elif pred == 0 and targ == 0:\n",
    "#                 true_neg.append(1)\n",
    "            \n",
    "#         elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "#             wrong_preds.append(1)\n",
    "            \n",
    "#             if pred == 1 and targ == 0:\n",
    "#                 false_pos.append(1)\n",
    "#             elif pred == 0 and targ == 1:\n",
    "#                 false_neg.append(1)\n",
    "    \n",
    "#     num_correct_preds = len(correct_preds)\n",
    "#     num_wrong_preds = len(wrong_preds)\n",
    "#     num_true_pos = len(true_pos)\n",
    "#     num_true_neg = len(true_neg)\n",
    "#     num_false_pos = len(false_pos)\n",
    "#     num_false_neg = len(false_neg)\n",
    "    \n",
    "#     # print('Correct preds: ' + str(num_correct_preds))\n",
    "#     # print('Wrong preds: ' + str(num_wrong_preds))\n",
    "#     # print('True pos: ' + str(num_true_pos))\n",
    "#     # print('True neg: ' + str(num_true_neg))\n",
    "#     # print('False pos: ' + str(num_false_pos))\n",
    "#     # print('False neg: ' + str(num_false_neg))\n",
    "    \n",
    "#     # print('Threshold: ' + str(threshold))\n",
    "#     # print('Correct preds: ' + str(num_correct_preds))\n",
    "#     # print('Wrong preds: ' + str(num_wrong_preds))\n",
    "#     # print('True pos: ' + str(num_true_pos))\n",
    "#     # print('True neg: ' + str(num_true_neg))\n",
    "#     # print('False pos: ' + str(num_false_pos))\n",
    "#     # print('False neg: ' + str(num_false_neg))\n",
    "    \n",
    "#     accuracy = num_correct_preds / num_preds\n",
    "#     accuracy_per = (num_correct_preds / num_preds) * 100\n",
    "#     # print('Accuracy: ' + str(accuracy_per) + '%')\n",
    "    \n",
    "#     if num_true_pos == 0  and num_false_pos == 0:\n",
    "#         precision = 0\n",
    "#     else:\n",
    "#         precision = num_true_pos / (num_true_pos + num_false_pos)\n",
    "    \n",
    "#     if num_true_pos == 0 and num_false_neg == 0:\n",
    "#         recall = 0\n",
    "#     else:\n",
    "#         recall = num_true_pos / (num_true_pos + num_false_neg)\n",
    "    \n",
    "#     if precision + recall == 0:\n",
    "#         F1 = 0\n",
    "#     else:\n",
    "#         F1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "#     accuracies.append(accuracy)\n",
    "#     accuracies_per.append(accuracy_per)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     F1s.append(F1)\n",
    "\n",
    "# # print('Accuracies')\n",
    "# # print(accuracies)\n",
    "# # print('Precisions')\n",
    "# # print(precisions)\n",
    "# # print('Recalls')\n",
    "# # print(recalls)\n",
    "# # print('F1s')\n",
    "# # print(F1s)\n",
    "\n",
    "# np.savetxt('realdata_accuracies_percentage_txt.txt', accuracies_per)\n",
    "# np.savetxt('realdata_thresholds_txt.txt', thresholds)\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# # plt.scatter(thresholds,accuracies)\n",
    "# plt.plot(thresholds, accuracies_per, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('Accuracy (%)', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,100)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('Accuracy Percentage', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_accuracies_' + str(num_test) + '.png', format='PNG')\n",
    "# # plt.savefig('/Users/sydneydybing/Documents/AGU_2021/Figures/6_accuracies_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/accuracies_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/accuracies_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# plt.plot(thresholds, precisions, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('Precision', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('Precision', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_precisions_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/precisions_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/precisions_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# plt.plot(thresholds, recalls, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('Recall', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('Recall', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_recalls_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/recalls_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/recalls_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# plt.figure(figsize = (8,5), dpi = 300)\n",
    "# plt.plot(thresholds, F1s, linewidth = 2)\n",
    "# plt.xlabel('Threshold', fontsize = 18)\n",
    "# plt.ylabel('F1', fontsize = 18)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.title('F1', fontsize = 18)\n",
    "# # plt.savefig(local_dir + 'plots/' + name + '/6_F1s_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/9_realdata_classify_stats/F1s_realdata.png', format='PNG')\n",
    "# # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/7_classify_stats/F1s_' + str(num_test) + '.png', format='PNG')\n",
    "# plt.close()\n",
    "\n",
    "# # ##### -------------------- GAUSSIAN PEAK POSITION TEST -------------------- #####\n",
    "\n",
    "# # print('DOING PEAK POSITION TESTS')\n",
    "\n",
    "# # # print(target[4])\n",
    "# # # print(test_predictions[4])\n",
    "\n",
    "# # thresholds = np.array([0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])\n",
    "\n",
    "# # # threshold = 0.2\n",
    "\n",
    "# # iterate = np.arange(0,len(realtest_predictions),1)\n",
    "# # s = 0\n",
    "\n",
    "# # fig = plt.subplots(nrows = 3, ncols = 4, figsize = (18,14), dpi = 400)\n",
    "# # # fig = plt.subplots(nrows = 3, ncols = 4, figsize = (18,14))\n",
    "# # plt.suptitle('Target vs. Prediction Samples Off per Threshold', fontsize = 20)\n",
    "\n",
    "# # for idx in range(len(thresholds)):\n",
    "    \n",
    "# #     threshold = thresholds[idx]\n",
    "\n",
    "# #     pred_binary = np.zeros(len(realtest_predictions))\n",
    "# #     iterate = np.arange(0,len(realtest_predictions),1)\n",
    "    \n",
    "# #     for k in iterate:\n",
    "# #         i = np.where(realtest_predictions[k] >= threshold)[0]\n",
    "# #         if len(i) == 0:\n",
    "# #             pred_binary[k] = 0\n",
    "# #         elif len(i) > 0:\n",
    "# #             pred_binary[k] = 1\n",
    "    \n",
    "# #     # print('Predictions: ')\n",
    "# #     # print(pred_binary)\n",
    "    \n",
    "# #     # Convert the target arrays to single ones and zeroes\n",
    "    \n",
    "# #     targ_binary = np.zeros(len(gauss_target)) # Need to make this ones at indices in rows_w_eqs\n",
    "    \n",
    "# #     for idx in range(len(targ_binary)):\n",
    "        \n",
    "# #         if idx in rows_w_eqs:\n",
    "            \n",
    "# #             targ_binary[idx] = 1\n",
    "    \n",
    "# #     # print('Targets: ')\n",
    "# #     # print(targ_binary)\n",
    "    \n",
    "# #     signals = []\n",
    "    \n",
    "# #     for i in iterate:\n",
    "# #         pred = pred_binary[i]\n",
    "# #         targ = targ_binary[i]\n",
    "        \n",
    "# #         # print(pred)\n",
    "# #         # print(targ)\n",
    "        \n",
    "# #         if pred == 1 and targ == 1: # True positive, there was a signal and it found it\n",
    "# #             signals.append(i) # Grab index from list of events that are correct and have a pick\n",
    "# #         else:\n",
    "# #             pass\n",
    "    \n",
    "# #     # print(signals)\n",
    "    \n",
    "# #     samples_off_list = []\n",
    "    \n",
    "# #     for index in signals:\n",
    "        \n",
    "# #         # Find the peak and then the index where that peak is and compare \n",
    "        \n",
    "# #         # print('----------------------')\n",
    "# #         # print('Signal number: ' + str(index))\n",
    "        \n",
    "# #         target_max_idx = np.argmax(gauss_target[index])\n",
    "# #         # print('Target: ' + str(target_max_idx))\n",
    "        \n",
    "# #         pred_max_idx = np.argmax(realtest_predictions[index])\n",
    "# #         # print('Prediction: ' + str(pred_max_idx))\n",
    "        \n",
    "# #         samples_off = np.abs(pred_max_idx - target_max_idx)\n",
    "# #         # print('Samples off: ' + str(samples_off))\n",
    "# #         samples_off_list.append(samples_off)\n",
    "        \n",
    "# #     # print(samples_off_list)\n",
    "    \n",
    "# #     plt.subplot(3,4,idx+1)\n",
    "# #     plt.hist(samples_off_list, bins=128, range=(0,128), label = 'Threshold: ' + str(threshold))\n",
    "# #     plt.xlim(0,128)\n",
    "# #     plt.ylim(0,300)\n",
    "# #     plt.legend()\n",
    "# #     plt.grid(which = 'major', color = 'lightgray')\n",
    "# #     plt.subplots_adjust(hspace = 0, wspace = 0)\n",
    "\n",
    "# #     if idx == 0:\n",
    "# #         plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "    \n",
    "# #     elif idx == 4:\n",
    "# #         plt.ylabel('Number of examples in bin')\n",
    "# #         plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "# #         plt.yticks([0, 50, 100, 150, 200, 250])\n",
    "        \n",
    "# #     elif idx == 8:\n",
    "# #         plt.yticks([0, 50, 100, 150, 200, 250])\n",
    "        \n",
    "# #     elif idx == 9:\n",
    "# #         plt.xlabel('Numbers of samples off target position')\n",
    "# #         plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "        \n",
    "# #     elif idx == 10:     \n",
    "# #         plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "        \n",
    "# #     else:\n",
    "# #         plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "# #         plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "    \n",
    "# #     plt.subplot(3,4,12)\n",
    "# #     plt.tick_params(axis = 'x', which = 'both', bottom = False, labelbottom = False)\n",
    "# #     plt.tick_params(axis = 'y', which = 'both', left = False, labelleft = False)\n",
    "\n",
    "# # # plt.savefig(local_dir + 'plots/' + name + '/7_histogram.png', format='PNG')\n",
    "# # plt.savefig('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/figures/10_realdata_peakpos_stats/histogram.png', format='PNG')\n",
    "# # # plt.savefig('/home/sdybing/GNSS_project/' + name + '/figures/8_peakpos_stats/histogram.png', format='PNG')\n",
    "# # plt.close()\n",
    "\n",
    "# # ##### -------------------- METADATA ANALYSIS -------------------- #####\n",
    "\n",
    "# # print('DOING REAL METADATA ANALYSIS')\n",
    "\n",
    "# # print(stack_data.shape) # the data\n",
    "# # print(norm_real_meta_data.shape) # the metadata\n",
    "# # print(realtest_predictions.shape) # the model's predictions about the data\n",
    "\n",
    "# # # np.save('batch_out_11_10_21.npy', batch_out)  \n",
    "# # # np.save('test_preds_11_10_21.npy', test_predictions)  \n",
    "# # # np.save('target_11_10_21.npy', target)\n",
    "\n",
    "# # # print(batch_out[0])\n",
    "# # # print(metadata[0][0])\n",
    "# # # print(test_predictions[0])\n",
    "\n",
    "# # # print(metadata)\n",
    "\n",
    "# # zeros = np.zeros((realtest_predictions.shape[0],1))\n",
    "# # analysis_array = np.c_[norm_real_meta_data,zeros]\n",
    "\n",
    "# # # Metadata columns: station, date, start time, end time, counter, gauss position, pgd, SNR N component, SNR E, SNR Z\n",
    "\n",
    "# # # print(analysis_array.shape)\n",
    "\n",
    "# # for i in range(len(stack_data)):\n",
    "    \n",
    "# #     # print(i)\n",
    "    \n",
    "# #     # print(metadata[i])\n",
    "\n",
    "# #     if norm_real_meta_data[i][5] == 'nan':\n",
    "# #         # print(str(i) + ' is not an earthquake')\n",
    "# #         # analysis_array[i][3] = 'nan'\n",
    "        \n",
    "# #         threshold = 0.16\n",
    "        \n",
    "# #         # True positive, true negative, false positive, false negative\n",
    "        \n",
    "# #         # print('Threshold: ' + str(threshold))\n",
    "    \n",
    "# #         # Convert the predictions arrays to single ones and zeroes\n",
    "        \n",
    "# #         p = np.where(test_predictions[i] >= threshold)[0]\n",
    "# #         if len(p) == 0:\n",
    "# #             pred_binary = 0\n",
    "# #         elif len(p) > 0:\n",
    "# #             pred_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Prediction: ')\n",
    "# #         #     print(pred_binary)\n",
    "        \n",
    "# #         # # Convert the target arrays to single ones and zeroes\n",
    "        \n",
    "# #         t = np.where(target[i] > 0)[0]\n",
    "# #         if len(t) == 0:\n",
    "# #             targ_binary = 0\n",
    "# #         elif len(t) > 0:\n",
    "# #             targ_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Target: ')\n",
    "# #         #     print(targ_binary)\n",
    "        \n",
    "# #         pred = pred_binary\n",
    "# #         targ = targ_binary\n",
    "        \n",
    "# #         if pred == targ: # add one to list of correct predictions if matching\n",
    "# #             # correct_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 1:\n",
    "# #                 result = 'true pos'\n",
    "# #             elif pred == 0 and targ == 0:\n",
    "# #                 result = 'true neg'\n",
    "            \n",
    "# #         elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "# #             # wrong_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 0:\n",
    "# #                 result = 'false pos'\n",
    "# #             elif pred == 0 and targ == 1:\n",
    "# #                 result = 'false neg'\n",
    "        \n",
    "# #         analysis_array[i][3] = result\n",
    "    \n",
    "# #     else:\n",
    "# #         # print(str(i) + ' is an earthquake')\n",
    "        \n",
    "# #         rupt_num = metadata[i][0]\n",
    "# #         station = metadata[i][1]\n",
    "# #         mag = metadata[i][2]\n",
    "        \n",
    "# #         # print(rupt_num)\n",
    "# #         # print(station)\n",
    "# #         # print(mag)\n",
    "        \n",
    "# #         # print(batch_out[i])\n",
    "# #         # print(test_predictions[i])\n",
    "# #         # plt.plot(test_predictions[i])\n",
    "# #         # plt.show()\n",
    "        \n",
    "# #         threshold = 0.2\n",
    "        \n",
    "# #         # True positive, true negative, false positive, false negative\n",
    "        \n",
    "# #         # print('Threshold: ' + str(threshold))\n",
    "    \n",
    "# #         # Convert the predictions arrays to single ones and zeroes\n",
    "        \n",
    "# #         p = np.where(test_predictions[i] >= threshold)[0]\n",
    "# #         if len(p) == 0:\n",
    "# #             pred_binary = 0\n",
    "# #         elif len(p) > 0:\n",
    "# #             pred_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Prediction: ')\n",
    "# #         #     print(pred_binary)\n",
    "        \n",
    "# #         # # Convert the target arrays to single ones and zeroes\n",
    "        \n",
    "# #         t = np.where(target[i] > 0)[0]\n",
    "# #         if len(t) == 0:\n",
    "# #             targ_binary = 0\n",
    "# #         elif len(t) > 0:\n",
    "# #             targ_binary = 1\n",
    "        \n",
    "# #         # if i == 0:\n",
    "# #         #     print('Target: ')\n",
    "# #         #     print(targ_binary)\n",
    "        \n",
    "# #         pred = pred_binary\n",
    "# #         targ = targ_binary\n",
    "        \n",
    "# #         if pred == targ: # add one to list of correct predictions if matching\n",
    "# #             # correct_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 1:\n",
    "# #                 result = 'true pos'\n",
    "# #             elif pred == 0 and targ == 0:\n",
    "# #                 result = 'true neg'\n",
    "            \n",
    "# #         elif pred != targ: # add ones to list of incorrect predictions if not matching\n",
    "# #             # wrong_preds.append(1)\n",
    "            \n",
    "# #             if pred == 1 and targ == 0:\n",
    "# #                 result = 'false pos'\n",
    "# #             elif pred == 0 and targ == 1:\n",
    "# #                 result = 'false neg'\n",
    "        \n",
    "# #         analysis_array[i][3] = result\n",
    "    \n",
    "# # print(analysis_array)\n",
    "# # print(analysis_array.shape)\n",
    "            \n",
    "# # # # np.save('/home/sdybing/GNSS_project/' + name + 'testing_for_analysis.npy', analysis_array) # VAL\n",
    "# # np.save('/Users/sydneydybing/GNSS-CNN_repo/GNSS-CNN/' + name + '/testing_for_analysis.npy', analysis_array) # LAP        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
