{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ca495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project = 'july6'\n",
    "traindate = '2023-08-26'\n",
    "testdate = '2023-08-27'\n",
    "traindate_path = '/home/sdybing/gnss-picker/cnn_models_outputs/' + project + '_fq_train/models/traindate_' + traindate + '/'\n",
    "test_outputs_path = traindate_path + 'data/'\n",
    "figure_save_dir = traindate_path + 'figures/'\n",
    "\n",
    "fqtest_data = np.load(test_outputs_path + testdate + '_fqtest_orig_data.npy')\n",
    "fqtest_metadata = np.load(test_outputs_path + testdate + '_fqtest_metadata.npy')\n",
    "fqtest_target = np.load(test_outputs_path + testdate + '_fqtest_target.npy')\n",
    "fqtest_predictions = np.load(test_outputs_path + testdate + '_fqtest_predictions.npy')\n",
    "\n",
    "results = np.load(test_outputs_path + 'fqtest_metadata_withresults.npy')\n",
    "\n",
    "num_fqtest = len(fqtest_predictions)\n",
    "best_thresh = 0.13 # From code 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f1e7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nan'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqtest_metadata[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e7a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91598,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating PGDs from the waveforms\n",
    "\n",
    "pgds = []\n",
    "\n",
    "for i in range(len(fqtest_data)):\n",
    "    \n",
    "    fqornot = fqtest_metadata[i,0]\n",
    "    if fqornot != 'nan':\n",
    "        n_data = fqtest_data[i,:,0]\n",
    "        e_data = fqtest_data[i,:,1]\n",
    "        z_data = fqtest_data[i,:,2]\n",
    "        pgd = np.max(np.sqrt((n_data)**2+(e_data)**2+(z_data)**2))\n",
    "        pgds.append(pgd)\n",
    "    else:\n",
    "        pgds.append('nan')\n",
    "\n",
    "print(np.array(pgds).shape)\n",
    "np.save(test_outputs_path + 'fqtest_data_pgds.npy', np.array(pgds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8f42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91598\n",
      "91598\n",
      "91598\n"
     ]
    }
   ],
   "source": [
    "# Calculating SNRs from the waveforms using the target Gaussian peak as the arrival time\n",
    "\n",
    "targets_count = []\n",
    "\n",
    "SNRs_N = []\n",
    "SNRs_E = []\n",
    "SNRs_Z = []\n",
    "\n",
    "for idx in range(len(fqtest_target)):\n",
    "    \n",
    "    target_min = min(fqtest_target[idx,:])\n",
    "    target_max = max(fqtest_target[idx,:])\n",
    "    target_range = target_max - target_min\n",
    "    \n",
    "    if target_range != 0:\n",
    "        target_max_idx = np.argmax(fqtest_target[idx,:])\n",
    "        targets_count.append(target_max_idx)\n",
    "    \n",
    "        p_arrival_index = int(target_max_idx) # The index in the sample that the P-wave arrives at\n",
    "        \n",
    "        '''\n",
    "        In this section, I calculate the signal-to-noise ratio of the data. I \n",
    "        aim to use a window of 20 seconds before the P-wave arrival time as the \n",
    "        noise, and a window of 20 seconds after the P-wave arrival time as the \n",
    "        signal. I take the standard deviation of these segments and divide \n",
    "        signal/noise (or after/before) to get the SNR.\n",
    "        \n",
    "        Sometimes the P-wave arrival time is too close to the start or end of the\n",
    "        sample, and this causes issues. I've added conditions for these cases.\n",
    "        '''\n",
    "        \n",
    "        preeq_std_end = p_arrival_index # The end of the 20 second 'noise' section before the earthquake is the P-wave arrival index\n",
    "        \n",
    "        if preeq_std_end <= 10: # Ask Diego if this is reasonable # Try 10\n",
    "        \n",
    "            # If P-wave pick is at zero - can't calculate a pre-eq standard deviation. \n",
    "            # OR the P-wave pick is too close to zero, it throws off the SNR values by a LOT.\n",
    "            \n",
    "            SNR_N = 'nan' # Just skip it (at least 10 cases for Z component with weird SNRs - one over 10,000!)\n",
    "            SNR_E = 'nan'\n",
    "            SNR_Z = 'nan'\n",
    "        \n",
    "        elif preeq_std_end > 10 and preeq_std_end <= 20: # If the pre-earthquake noise window is smaller than 20 seconds...\n",
    "            \n",
    "            preeq_std_start = 0\n",
    "            \n",
    "            posteq_std_start = p_arrival_index # Start the section for the \"signal\" at the P-wave arrival index\n",
    "            posteq_std_end = posteq_std_start + 20\n",
    "            # posteq_std_end = posteq_std_start + p_arrival_index # If the window before is less than 20 because the arrival time is less than 20, this makes the window after that same length\n",
    "            \n",
    "            std_before_N = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,0]) # Take the standard deviation of the sections for each component\n",
    "            std_after_N = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,0])\n",
    "            std_before_E = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,1])\n",
    "            std_after_E = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,1])\n",
    "            std_before_Z = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,2])\n",
    "            std_after_Z = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,2])\n",
    "            \n",
    "            if std_before_N == 0 or std_before_E == 0 or std_before_Z == 0: # If any of the denominators are zeros, we get 'inf' in the results\n",
    "                \n",
    "                SNR_N = 'nan' # Skip 'em\n",
    "                SNR_E = 'nan'\n",
    "                SNR_Z = 'nan'\n",
    "                \n",
    "            else: # Calculate the SNR\n",
    "                \n",
    "                SNR_N = std_after_N / std_before_N\n",
    "                SNR_E = std_after_E / std_before_E\n",
    "                SNR_Z = std_after_Z / std_before_Z\n",
    "        \n",
    "        elif preeq_std_end > 20 and preeq_std_end <= 108: # Standard case where the P-wave arrival is nicely in the middle somewhere\n",
    "            \n",
    "            preeq_std_start = preeq_std_end - 20\n",
    "            \n",
    "            posteq_std_start = p_arrival_index\n",
    "            posteq_std_end = posteq_std_start + 20\n",
    "        \n",
    "            std_before_N = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,0]) # Take the standard deviation of the sections for each component\n",
    "            std_after_N = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,0])\n",
    "            std_before_E = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,1])\n",
    "            std_after_E = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,1])\n",
    "            std_before_Z = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,2])\n",
    "            std_after_Z = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,2])\n",
    "            \n",
    "            if std_before_N == 0 or std_before_E == 0 or std_before_Z == 0:\n",
    "                \n",
    "                SNR_N = 'nan'\n",
    "                SNR_E = 'nan'\n",
    "                SNR_Z = 'nan'\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                SNR_N = std_after_N / std_before_N\n",
    "                SNR_E = std_after_E / std_before_E\n",
    "                SNR_Z = std_after_Z / std_before_Z\n",
    "            \n",
    "        elif preeq_std_end > 108 and preeq_std_end < 128: # End edge case - the \"signal\" period is less than 20 seconds long\n",
    "            \n",
    "            preeq_std_start = preeq_std_end - 20\n",
    "            \n",
    "            posteq_std_start = p_arrival_index # Should the below be 127 instead??\n",
    "            posteq_std_end = posteq_std_start + (128 - p_arrival_index) # Make the signal period end at the end of the sample at 128 to avoid errors\n",
    "        \n",
    "            std_before_N = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,0]) # Take the standard deviation of the sections for each component\n",
    "            std_after_N = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,0])\n",
    "            std_before_E = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,1])\n",
    "            std_after_E = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,1])\n",
    "            std_before_Z = np.std(fqtest_data[idx,preeq_std_start:preeq_std_end,2])\n",
    "            std_after_Z = np.std(fqtest_data[idx,posteq_std_start:posteq_std_end,2])\n",
    "            \n",
    "            if std_before_N == 0 or std_before_E == 0 or std_before_Z == 0:\n",
    "                \n",
    "                SNR_N = 'nan'\n",
    "                SNR_E = 'nan'\n",
    "                SNR_Z = 'nan'\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                SNR_N = std_after_N / std_before_N\n",
    "                SNR_E = std_after_E / std_before_E\n",
    "                SNR_Z = std_after_Z / std_before_Z\n",
    "            \n",
    "        else: # Covers if the pick is exactly at 128, the end of the sample.\n",
    "            \n",
    "            # Can't get a post-eq std because the earthquake arrives at the end of the sample\n",
    "            \n",
    "            SNR_N = 'nan' # Skip 'em (5 cases)\n",
    "            SNR_E = 'nan'\n",
    "            SNR_Z = 'nan'\n",
    "            \n",
    "        '''\n",
    "        Add the calculated SNRs (or 'nan's for issues) to the lists.\n",
    "        '''\n",
    "        \n",
    "        # if SNR_N == 0:\n",
    "            \n",
    "        #     print(idx)\n",
    "            \n",
    "        SNRs_N.append(SNR_N)\n",
    "        SNRs_E.append(SNR_E)\n",
    "        SNRs_Z.append(SNR_Z)\n",
    "        \n",
    "    elif target_range == 0:\n",
    "        \n",
    "        SNRs_N.append('nan')\n",
    "        SNRs_E.append('nan')\n",
    "        SNRs_Z.append('nan')\n",
    "\n",
    "print(len(SNRs_N))\n",
    "print(len(SNRs_E))\n",
    "print(len(SNRs_Z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f048ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(test_outputs_path + 'fqtest_data_SNRs_N.npy', np.array(SNRs_N))\n",
    "np.save(test_outputs_path + 'fqtest_data_SNRs_E.npy', np.array(SNRs_E))\n",
    "np.save(test_outputs_path + 'fqtest_data_SNRs_Z.npy', np.array(SNRs_Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2fd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1242609/1667123257.py:27: RuntimeWarning: divide by zero encountered in log10\n",
      "  logsnrs = np.log10(testsnrs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN mags removed: 45799\n",
      "Number of good mags left: 45799\n",
      "Number of NaN PGDs removed: 45799\n",
      "Number of good PGDs left: 45799\n",
      "Number of NaN SNRs removed: 50144\n",
      "Number of good SNRs left: 41454\n"
     ]
    }
   ],
   "source": [
    "# Plots to check and make sure stuff looks right\n",
    "\n",
    "testmags = []\n",
    "for ii in range(len(fqtest_metadata)):\n",
    "    mag = fqtest_metadata[ii,2]\n",
    "    if mag == 'nan':\n",
    "        testmags.append(np.nan)\n",
    "    else:\n",
    "        testmags.append(float(mag))\n",
    "        \n",
    "testsnrs = []\n",
    "for iii in range(len(SNRs_N)):\n",
    "    snr = SNRs_N[iii]\n",
    "    if snr == 'nan':\n",
    "        testsnrs.append(np.nan)\n",
    "    else:\n",
    "        testsnrs.append(float(snr))\n",
    "        \n",
    "testpgds = []\n",
    "for iiii in range(len(pgds)):\n",
    "    pgd = pgds[iiii]\n",
    "    if pgd == 'nan':\n",
    "        testpgds.append(np.nan)\n",
    "    else:\n",
    "        testpgds.append(float(pgd))\n",
    "\n",
    "logsnrs = np.log10(testsnrs)\n",
    "logpgds = np.log10(testpgds)\n",
    "\n",
    "# Remove NaNs to be able to make this quick plot\n",
    "fixmags = []\n",
    "numremovedmags = []\n",
    "for iv in range(len(testmags)):\n",
    "    mag = testmags[iv]\n",
    "    if np.isnan(mag):\n",
    "        numremovedmags.append(1)\n",
    "    else:\n",
    "        fixmags.append(mag)    \n",
    "print('Number of NaN mags removed: ' + str(len(numremovedmags)))\n",
    "print('Number of good mags left: ' + str(len(fixmags)))\n",
    "\n",
    "fixpgds = []\n",
    "numremovedpgds = []\n",
    "for iv in range(len(logpgds)):\n",
    "    pgd = logpgds[iv]\n",
    "    if np.isnan(pgd):\n",
    "        numremovedpgds.append(1)\n",
    "    else:\n",
    "        fixpgds.append(pgd)    \n",
    "print('Number of NaN PGDs removed: ' + str(len(numremovedpgds)))\n",
    "print('Number of good PGDs left: ' + str(len(fixpgds)))\n",
    "\n",
    "fixsnrs = []\n",
    "numremovedsnrs = []\n",
    "for iv in range(len(logsnrs)):\n",
    "    snr = logsnrs[iv]\n",
    "    if np.isnan(snr):\n",
    "        numremovedsnrs.append(1)\n",
    "    elif np.isinf(snr):\n",
    "        numremovedsnrs.append(1)\n",
    "    else:\n",
    "        fixsnrs.append(snr) \n",
    "print('Number of NaN SNRs removed: ' + str(len(numremovedsnrs)))\n",
    "print('Number of good SNRs left: ' + str(len(fixsnrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d5a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(nrows = 2, ncols = 2, figsize = (15,10), dpi = 300, facecolor = 'white')\n",
    "plt.suptitle('PGD and SNR distribution in FQ testing dataset', fontsize = 20)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(fixpgds, bins = 50, color = 'deepskyblue', edgecolor = 'black')\n",
    "plt.xlabel('Log PGD (m)', fontsize = 16)\n",
    "plt.ylabel('Count', fontsize = 16)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(testmags, logpgds, s = 0.5, color = 'deepskyblue')\n",
    "plt.xlabel('Magnitude', fontsize = 16)\n",
    "plt.ylabel('Log PGD (m)', fontsize = 16)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(fixsnrs, bins = 50, color = 'limegreen', edgecolor = 'black')\n",
    "plt.xlabel('Log SNR (N-S component)', fontsize = 16)\n",
    "plt.ylabel('Count', fontsize = 16)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(testmags, logsnrs, s = 0.5, color = 'limegreen')\n",
    "plt.xlabel('Magnitude', fontsize = 16)\n",
    "plt.ylabel('Log SNR (N-S component)', fontsize = 16)\n",
    "\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '9a_pgd_and_snr_distribution_fqtest_data.png', format = 'PNG')\n",
    "plt.close();\n",
    "\n",
    "plt.figure(figsize = (8,5), facecolor = 'white', dpi = 300)\n",
    "plt.title('Magnitude distribution in FQ testing dataset', fontsize = 18)\n",
    "plt.hist(fixmags, bins = 50, color = 'orange', edgecolor = 'black')\n",
    "plt.xlabel('Magnitude', fontsize = 16)\n",
    "plt.ylabel('Count', fontsize = 16)\n",
    "\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '9b_magnitude_distribution_fqtest_data.png', format = 'PNG')\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74627fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45799\n",
      "12474\n"
     ]
    }
   ],
   "source": [
    "# Make a list of indices for rows in the testing dataset that actually have earthquakes\n",
    "# and for rows that the CNN correctly found those earthquakes\n",
    "\n",
    "rows_w_eqs = []\n",
    "correct_eq_inds = []\n",
    "\n",
    "for idx in range(len(results)):\n",
    "    \n",
    "    if results[idx,0] == 'nan':\n",
    "        pass\n",
    "    else:\n",
    "        rows_w_eqs.append(idx)\n",
    "    \n",
    "    if results[idx,3] == 'true pos':\n",
    "        correct_eq_inds.append(idx)\n",
    "\n",
    "print(len(rows_w_eqs))\n",
    "print(len(correct_eq_inds))\n",
    "\n",
    "np.save(test_outputs_path + 'fqtest_data_rows_w_eqs.npy', np.array(rows_w_eqs))\n",
    "np.save(test_outputs_path + 'fqtest_data_rows_w_truepos_result.npy', np.array(correct_eq_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ced28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the PGDs and SNRs to the results metadata array\n",
    "\n",
    "pgds_vector = np.array(pgds).reshape(len(pgds),1) \n",
    "a = np.append(results, pgds_vector, axis = 1) \n",
    "# Now contains rupture name, station name, magnitude, result, and PGD\n",
    "\n",
    "SNR_N_vector = np.array(SNRs_N).reshape(len(SNRs_N),1)\n",
    "SNR_E_vector = np.array(SNRs_E).reshape(len(SNRs_E),1)\n",
    "SNR_Z_vector = np.array(SNRs_Z).reshape(len(SNRs_Z),1)\n",
    "\n",
    "b = np.append(a, SNR_N_vector, axis = 1)\n",
    "c = np.append(b, SNR_E_vector, axis = 1)\n",
    "new_meta_array = np.append(c, SNR_Z_vector, axis = 1)\n",
    "\n",
    "# New metadata array columns:\n",
    "\n",
    "# 0: FQ rupture name\n",
    "# 1: station name\n",
    "# 2: magnitude\n",
    "# 3: result (true pos, etc.)\n",
    "# 4: PGD\n",
    "# 5: SNR_N\n",
    "# 6: SNR_E\n",
    "# 7: SNR_Z\n",
    "\n",
    "np.save(test_outputs_path + 'fqtest_metadata_with_results_pgd_snr.npy', new_meta_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2c3842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average magnitude of all FakeQuakes: 5.81\n",
      "Average magnitude of FakeQuakes the CNN correctly found: 6.7\n"
     ]
    }
   ],
   "source": [
    "all_eq_mags = np.asfarray(new_meta_array[rows_w_eqs, 2]) # The magnitudes of all of the earthquakes\n",
    "correct_eq_mags = np.asfarray(new_meta_array[correct_eq_inds, 2]) # The magnitudes of all the earthquakes the CNN found\n",
    "\n",
    "all_eq_avg_mag = np.mean(all_eq_mags)\n",
    "correct_eq_avg_mag = np.mean(correct_eq_mags)\n",
    "\n",
    "print('Average magnitude of all FakeQuakes: ' + str(round((all_eq_avg_mag),2)))\n",
    "print('Average magnitude of FakeQuakes the CNN correctly found: ' + str(round((correct_eq_avg_mag),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde485a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PGD of all FakeQuakes: 3.56 cm\n",
      "Average PGD of FakeQuakes the CNN correctly found: 5.13 cm\n"
     ]
    }
   ],
   "source": [
    "all_eq_pgds = np.asfarray(new_meta_array[rows_w_eqs, 4]) # The PGDs of all of the earthquakes\n",
    "correct_eq_pgds = np.asfarray(new_meta_array[correct_eq_inds, 4]) # The PGDs of all the earthquakes the CNN found\n",
    "\n",
    "all_eq_avg_PGD = np.mean(all_eq_pgds)\n",
    "correct_eq_avg_PGD = np.mean(correct_eq_pgds)\n",
    "\n",
    "print('Average PGD of all FakeQuakes: ' + str(round((all_eq_avg_PGD * 100),2)) + ' cm')\n",
    "print('Average PGD of FakeQuakes the CNN correctly found: ' + str(round((correct_eq_avg_PGD * 100),2)) + ' cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1137a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the edge cases in the loop above, there are some nans in our SNR vectors.\n",
    "# We can't calculate averages with nans, so we need to find the rows with nans and\n",
    "# just remove them for the sake of this calculation.\n",
    "\n",
    "h = np.where(new_meta_array[rows_w_eqs,5] == 'nan') # Finds nans in SNRN column for all earthquakes\n",
    "non_nan_rows_w_eqs = np.delete(rows_w_eqs, h) # Removes those rows\n",
    "\n",
    "j = np.where(new_meta_array[correct_eq_inds,5] == 'nan') # Finds nans in SNRN for the earthquakes the CNN found\n",
    "non_nan_correct_eq_inds = np.delete(correct_eq_inds, j) # Removes those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fcf4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average N-S component SNR of all FakeQuakes: 1.36\n",
      "Average N-S component SNR of FakeQuakes the CNN correctly found: 2.24\n",
      "-------------------------------------------------------------------\n",
      "Average E-W component SNR of all FakeQuakes: 1.46\n",
      "Average E-W component SNR of FakeQuakes the CNN correctly found: 2.61\n",
      "-------------------------------------------------------------------\n",
      "Average Z component SNR of all earthquakes: 1.07\n",
      "Average Z component SNR of earthquakes the CNN correctly found: 1.21\n"
     ]
    }
   ],
   "source": [
    "# Now I just grab the good SNRs out of the new metadata array and calculate the averages.\n",
    "\n",
    "all_eq_SNR_N = np.asfarray(new_meta_array[non_nan_rows_w_eqs, 5])\n",
    "all_eq_SNR_E = np.asfarray(new_meta_array[non_nan_rows_w_eqs, 6])\n",
    "all_eq_SNR_Z = np.asfarray(new_meta_array[non_nan_rows_w_eqs, 7])\n",
    "\n",
    "correct_eq_SNR_N = np.asfarray(new_meta_array[non_nan_correct_eq_inds, 5])\n",
    "correct_eq_SNR_E = np.asfarray(new_meta_array[non_nan_correct_eq_inds, 6])\n",
    "correct_eq_SNR_Z = np.asfarray(new_meta_array[non_nan_correct_eq_inds, 7])\n",
    "\n",
    "all_eq_SNR_N_avg = np.mean(all_eq_SNR_N)\n",
    "all_eq_SNR_E_avg = np.mean(all_eq_SNR_E)\n",
    "all_eq_SNR_Z_avg = np.mean(all_eq_SNR_Z)\n",
    "\n",
    "correct_eq_SNR_N_avg = np.mean(correct_eq_SNR_N)\n",
    "correct_eq_SNR_E_avg = np.mean(correct_eq_SNR_E)\n",
    "correct_eq_SNR_Z_avg = np.mean(correct_eq_SNR_Z)\n",
    "\n",
    "# print(len(all_eq_SNR_N))\n",
    "# print(len(correct_eq_SNR_N))\n",
    "\n",
    "print('Average N-S component SNR of all FakeQuakes: ' + str(round(all_eq_SNR_N_avg,2)))\n",
    "print('Average N-S component SNR of FakeQuakes the CNN correctly found: ' + str(round(correct_eq_SNR_N_avg,2)))\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "print('Average E-W component SNR of all FakeQuakes: ' + str(round(all_eq_SNR_E_avg,2)))\n",
    "print('Average E-W component SNR of FakeQuakes the CNN correctly found: ' + str(round(correct_eq_SNR_E_avg,2)))\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "print('Average Z component SNR of all earthquakes: ' + str(round(all_eq_SNR_Z_avg,2)))\n",
    "print('Average Z component SNR of earthquakes the CNN correctly found: ' + str(round(correct_eq_SNR_Z_avg,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92103e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
