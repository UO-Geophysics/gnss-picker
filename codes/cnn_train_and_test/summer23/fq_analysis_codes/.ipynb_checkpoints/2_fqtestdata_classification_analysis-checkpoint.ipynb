{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539ce257",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sydneydybing/gnss-picker/cnn_models_outputs/july6_fq_train/models/traindate_2023-08-26/data/2023-08-27_fqtest_orig_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m test_outputs_path \u001b[38;5;241m=\u001b[39m traindate_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m figure_save_dir \u001b[38;5;241m=\u001b[39m traindate_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigures/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m fqtest_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_outputs_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtestdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_fqtest_orig_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m fqtest_metadata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(test_outputs_path \u001b[38;5;241m+\u001b[39m testdate \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fqtest_metadata.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m fqtest_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(test_outputs_path \u001b[38;5;241m+\u001b[39m testdate \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fqtest_target.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sydneydybing/gnss-picker/cnn_models_outputs/july6_fq_train/models/traindate_2023-08-26/data/2023-08-27_fqtest_orig_data.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project = 'july6'\n",
    "traindate = '2023-08-26'\n",
    "testdate = '2023-08-27'\n",
    "traindate_path = '/home/sdybing/gnss-picker/cnn_models_outputs/' + project + '_fq_train/models/traindate_' + traindate + '/'\n",
    "test_outputs_path = traindate_path + 'data/'\n",
    "figure_save_dir = traindate_path + 'figures/'\n",
    "\n",
    "fqtest_data = np.load(test_outputs_path + testdate + '_fqtest_orig_data.npy')\n",
    "fqtest_metadata = np.load(test_outputs_path + testdate + '_fqtest_metadata.npy')\n",
    "fqtest_target = np.load(test_outputs_path + testdate + '_fqtest_target.npy')\n",
    "fqtest_predictions = np.load(test_outputs_path + testdate + '_fqtest_predictions.npy')\n",
    "\n",
    "num_fqtest = len(fqtest_predictions)\n",
    "thresholds = np.arange(0, 1.005, 0.005)\n",
    "test_thresholds = [0, 0.005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa30d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "accuracies_per = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "F1s = []\n",
    "\n",
    "for ind in range(len(thresholds)):\n",
    "    \n",
    "    threshold = np.round(thresholds[ind],3)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Threshold: ' + str(threshold))\n",
    "    \n",
    "    iterate = np.arange(0,num_fqtest,1)\n",
    "    \n",
    "    # Convert the prediction arrays to 1s and 0s if the prediction Gaussian exceeded the threshold or not\n",
    "        \n",
    "    pred_binary = np.zeros(len(fqtest_predictions)) # Initialize the array with all zeros\n",
    "    for k in iterate:\n",
    "#         plt.plot(fqtest_predictions[k]) # The output \"Gaussian\" or straight line prediction\n",
    "#         plt.ylim(0,1)\n",
    "#         plt.show()\n",
    "        i = np.where(fqtest_predictions[k] >= threshold)[0]\n",
    "        if len(i) == 0: \n",
    "            pred_binary[k] = 0\n",
    "        elif len(i) > 0: # If anywhere in the prediction the Gaussian exceeds the threadshold, add a 1 to the pred_binary array for this prediction\n",
    "            pred_binary[k] = 1\n",
    "#     print('Predictions: ')\n",
    "#     print(pred_binary) \n",
    "    \n",
    "    # Convert the target arrays to 1s and 0s if the Gaussian exceeded the threshold or not (signal or noise)\n",
    "    \n",
    "    targ_binary = np.zeros(len(fqtest_target))\n",
    "    for k in iterate:\n",
    "        i = np.where(fqtest_target[k] > 0)[0]\n",
    "        if len(i) == 0:\n",
    "            targ_binary[k] = 0\n",
    "        elif len(i) > 0:\n",
    "            targ_binary[k] = 1\n",
    "#     print('Targets: ')\n",
    "#     print(targ_binary)\n",
    "    \n",
    "    # Calculating the accuracy, precision, recall, and F1\n",
    "    \n",
    "    num_preds = num_fqtest # Total number of predictions\n",
    "    correct_preds = []\n",
    "    wrong_preds = []\n",
    "    true_pos = []\n",
    "    true_neg = []\n",
    "    false_pos = []\n",
    "    false_neg = []\n",
    "    \n",
    "    for i in iterate:\n",
    "        \n",
    "        pred = pred_binary[i]\n",
    "        targ = targ_binary[i]\n",
    "        \n",
    "        if pred == targ: # Add one to list of correct predictions if matching\n",
    "            correct_preds.append(1)\n",
    "            \n",
    "            if pred == 1 and targ == 1: # True positive: there is an earthquake, and the model found it\n",
    "                true_pos.append(1)\n",
    "            elif pred == 0 and targ == 0: # True negative: there isn't an earthquake, and the model found just noise\n",
    "                true_neg.append(1)\n",
    "            \n",
    "        elif pred != targ: # Add ones to list of incorrect predictions if not matching\n",
    "            wrong_preds.append(1)\n",
    "            \n",
    "            if pred == 1 and targ == 0: # False positive: there isn't an earthquake, and the model thought it found one\n",
    "                false_pos.append(1)\n",
    "            elif pred == 0 and targ == 1: # False negative: there is an earthquake, and the model missed it\n",
    "                false_neg.append(1)\n",
    "    \n",
    "    num_correct_preds = len(correct_preds)\n",
    "    num_wrong_preds = len(wrong_preds)\n",
    "    num_true_pos = len(true_pos)\n",
    "    num_true_neg = len(true_neg)\n",
    "    num_false_pos = len(false_pos)\n",
    "    num_false_neg = len(false_neg)\n",
    "    \n",
    "    # print('Threshold: ' + str(threshold))\n",
    "    # print('Correct preds: ' + str(num_correct_preds))\n",
    "    # print('Wrong preds: ' + str(num_wrong_preds))\n",
    "    # print('True pos: ' + str(num_true_pos))\n",
    "    # print('True neg: ' + str(num_true_neg))\n",
    "    # print('False pos: ' + str(num_false_pos))\n",
    "    # print('False neg: ' + str(num_false_neg))\n",
    "    \n",
    "    accuracy = num_correct_preds / num_preds\n",
    "    accuracy_per = (num_correct_preds / num_preds) * 100\n",
    "    print('Accuracy: ' + str(accuracy_per) + '%')\n",
    "    \n",
    "    if num_true_pos == 0  and num_false_pos == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = num_true_pos / (num_true_pos + num_false_pos)\n",
    "    \n",
    "    if num_true_pos == 0 and num_false_neg == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = num_true_pos / (num_true_pos + num_false_neg)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    accuracies_per.append(accuracy_per)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    F1s.append(F1)\n",
    "\n",
    "# print('Accuracies')\n",
    "# print(accuracies)\n",
    "# print('Precisions')\n",
    "# print(precisions)\n",
    "# print('Recalls')\n",
    "# print(recalls)\n",
    "# print('F1s')\n",
    "# print(F1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b40096",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(test_outputs_path + 'accuracies_percentage_txt.txt', accuracies_per)\n",
    "np.savetxt(test_outputs_path + 'thresholds_txt.txt', thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01212a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold with highest accuracy\n",
    "\n",
    "acc0 = 0\n",
    "\n",
    "for idx in range(len(accuracies_per)):\n",
    "    acc = accuracies_per[idx]\n",
    "    if acc > acc0:\n",
    "        acc0 = acc\n",
    "        best_thresh = thresholds[idx] # Only updates when it hits a higher accuracy\n",
    "        \n",
    "print(best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb5c3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.plot(thresholds, accuracies_per, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('Accuracy (%)', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,100)\n",
    "plt.axvline(best_thresh, color = 'red', linestyle = '--', alpha = 0.6, label = 'Max accuracy at\\nthreshold of ' + str(best_thresh))\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Accuracy', fontsize = 18)\n",
    "plt.legend()\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '7_accuracies_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "plt.close();\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.plot(thresholds, precisions, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('Precision', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Precision', fontsize = 18)\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '7_precisions_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "plt.close();\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.plot(thresholds, recalls, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('Recall', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Recall', fontsize = 18)\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '7_recalls_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "plt.close();\n",
    "\n",
    "plt.figure(figsize = (8,5), dpi = 300)\n",
    "plt.plot(thresholds, F1s, linewidth = 2)\n",
    "plt.xlabel('Threshold', fontsize = 18)\n",
    "plt.ylabel('F1', fontsize = 18)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('F1', fontsize = 18)\n",
    "# plt.show();\n",
    "plt.savefig(figure_save_dir + '7_F1s_by_threshold.png', format = 'PNG', facecolor = 'white')\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae06cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
